{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Implementing Transformers for Text Generation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn to implement Transformers for text generation tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Implement Transformers for text generation tasks \n",
    "\n",
    "- Build, train, and evaluate Transformer models for text generation using TensorFlow and Keras \n",
    "\n",
    "- Apply text generation in real-world scenarios \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step instructions \n",
    "\n",
    "#### Step 1: Set up the environment \n",
    "\n",
    "- Import necessary libraries and load the data set \n",
    "\n",
    "- Preprocess the dataset for training \n",
    "\n",
    "In the following code: \n",
    "\n",
    "- Import TensorFlow and other necessary libraries. \n",
    "\n",
    "- Load the Shakespeare text dataset. \n",
    "\n",
    "- Preprocess the data set using the TextVectorization layer to convert text into integer sequences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow==2.16.2\n",
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "from tensorflow.keras.layers import TextVectorization \n",
    "from tensorflow.keras.utils import get_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "path_to_file = get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt') \n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8') \n",
    "\n",
    "# Preview the dataset \n",
    "print(text[:1000]) "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:07:09.214904Z",
     "start_time": "2025-10-27T18:07:09.082347Z"
    }
   },
   "source": [
    "# Preprocess the dataset\n",
    "# vocab_size is the max number of unique words (i.e. tokens) the model will know with any word outside the most common 10,000\n",
    "# mapped to an \"unknown\" token. The seq_length represents the context window the model will learn from with the transformer\n",
    "# \"seeing\" 100 tokens at a time.\n",
    "vocab_size = 10000 \n",
    "seq_length = 100 \n",
    "\n",
    "# Adapt TextVectorization to full text \n",
    "vectorizer = TextVectorization(max_tokens=vocab_size, output_mode='int') \n",
    "text_ds = tf.data.Dataset.from_tensor_slices([text]).batch(1) \n",
    "vectorizer.adapt(text_ds) \n",
    "\n",
    "# Vectorize the text \n",
    "vectorized_text = vectorizer([text])[0] \n",
    "print(\"Vectorized text shape:\", vectorized_text.shape) \n",
    "print(\"First 10 vectorized tokens:\", vectorized_text.numpy()[:10]) "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized text shape: (202646,)\n",
      "First 10 vectorized tokens: [ 89 270 138  36 982 144 673 125  16 106]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-27 11:07:09.171043: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create input and target sequences \n",
    "\n",
    "Generate input and target sequences for training the Transformer model. \n",
    "\n",
    "In the following code: \n",
    "\n",
    "- Define a function to generate input and target sequences. \n",
    "\n",
    "- Split the text data into sequences of the specified length. \n",
    "\n",
    "- Convert the sequences into TensorFlow tensors for training. \n",
    "\n",
    "Generative sequence: \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:16:53.558156Z",
     "start_time": "2025-10-27T18:16:53.353073Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "    Create overlapping input/target pairs for text generation training.\n",
    "\n",
    "    Strategy: Sliding window with 1-token shift between input and target\n",
    "    - Generates ~len(text) training examples (overlapping by seq_length-1 tokens)\n",
    "    - Each target sequence is input shifted right by 1 (next-token prediction)\n",
    "    - Model learns: given tokens at positions [i...i+99], predict [i+1...i+100]\n",
    "\n",
    "    Example with seq_length=3:\n",
    "        text = [10, 20, 30, 40, 50]\n",
    "        → input=[10,20,30], target=[20,30,40]\n",
    "        → input=[20,30,40], target=[30,40,50]\n",
    "    \"\"\"\n",
    "def create_sequences(text, seq_length):\n",
    "    input_seqs = [] \n",
    "    target_seqs = [] \n",
    "    for i in range(len(text) - seq_length): \n",
    "        input_seq = text[i:i + seq_length] \n",
    "        target_seq = text[i + 1:i + seq_length + 1] \n",
    "        input_seqs.append(input_seq) \n",
    "        target_seqs.append(target_seq) \n",
    "    return np.array(input_seqs), np.array(target_seqs) \n",
    "\n",
    "# Generate sequences \n",
    "X, Y = create_sequences(vectorized_text.numpy(), seq_length) \n",
    "\n",
    "# Check if sequences are correctly generated \n",
    "print(\"Number of sequences generated:\", len(X)) \n",
    "print(\"Sample input sequence:\", X[0] if len(X) > 0 else \"No sequences generated\") \n",
    "\n",
    "# Check if X and Y are not empty \n",
    "assert X.size > 0, \"Input data X is empty\" \n",
    "assert Y.size > 0, \"Target data Y is empty\" \n",
    "X = tf.convert_to_tensor(X) \n",
    "Y = tf.convert_to_tensor(Y) \n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences generated: 202546\n",
      "Sample input sequence: [  89  270  138   36  982  144  673  125   16  106   34  106  106   89\n",
      "  270    7   41   34 1286  344    4  200   64    4 3690   34 1286 1286\n",
      "   89  270   89    7   93 1187  225   12 2442  592    4    2  307   34\n",
      "   36 2655   36 2655   89  270   72   79  506   27    3   56   24 1390\n",
      "   57   40  161 2328  644    9 4980   34   32   54 2863  885   72   17\n",
      "   18  163  146  146  165  270   74  218   46  595   89  270   36   41\n",
      " 6739  172  595    2 1780   46   29 1323 5151   47   58 4151   79   39\n",
      "   60   58]\n",
      "Shape of X: (202546, 100)\n",
      "Shape of Y: (202546, 100)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Build the Transformer model \n",
    "\n",
    "Define the Transformer model architecture for text generation. \n",
    "\n",
    "In the following code: \n",
    "\n",
    "- Define the TransformerBlock class that includes multi-head attention and feedforward layers with normalization and dropout. \n",
    "\n",
    "- Define the TransformerModel class, including embedding, positional encoding, and multiple Transformer blocks. \n",
    "\n",
    "- Compile the Transformer model using the Adam optimizer and sparse categorical cross-entropy loss function. \n",
    "\n",
    "Transformer model: \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:17:41.746305Z",
     "start_time": "2025-10-27T18:17:41.729992Z"
    }
   },
   "source": [
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A single Transformer block - the fundamental building unit that gets stacked.\n",
    "\n",
    "    HIGH-LEVEL FLOW:\n",
    "    1. Multi-head self-attention (parallel processing of relationships)\n",
    "    2. Add & Normalize (residual connection + normalization to zero mean)\n",
    "    3. Feed-Forward Network with ReLU (non-linear transformation of each token)\n",
    "    4. Add & Normalize again (another residual connection + normalization)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # STEP 1 SETUP: Multi-head self-attention\n",
    "        # - num_heads: Number of parallel attention mechanisms (e.g., 4 heads process simultaneously)\n",
    "        # - key_dim: Dimension of query/key vectors for attention computation\n",
    "        # Each head learns different types of relationships between tokens\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "\n",
    "        # STEP 3 SETUP: Feed-Forward Network (FFN)\n",
    "        # Two Dense layers with ReLU activation between them\n",
    "        # - First Dense expands to ff_dim (512) with ReLU for non-linearity\n",
    "        # - Second Dense projects back to embed_dim (256)\n",
    "        # Processes each token's representation individually (not across tokens)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),  # ReLU introduces non-linearity\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "\n",
    "        # STEP 2 & 4 SETUP: Layer Normalization\n",
    "        # Normalizes to zero mean and unit variance across features\n",
    "        # Helps stabilize training in deep networks\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)  # After attention\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)  # After FFN\n",
    "\n",
    "        # Dropout for regularization (prevents overfitting)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Execute one complete transformer block pass.\n",
    "        inputs shape: (batch_size, seq_length, embed_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # STEP 1: Multi-Head Self-Attention\n",
    "        # Each token looks at all other tokens to understand context\n",
    "        # att(inputs, inputs) means: query=inputs, key=inputs, value=inputs (self-attention)\n",
    "        # Multiple heads process in parallel, learning different relationship types\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "\n",
    "        # STEP 2: Add & Normalize (First Residual Connection)\n",
    "        # inputs + attn_output: Residual/skip connection preserves original information\n",
    "        # Helps gradients flow during backprop (prevents vanishing gradients)\n",
    "        # layernorm1: Normalizes to zero mean and unit variance\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        # STEP 3: Feed-Forward Network\n",
    "        # Applies non-linear transformation to each token independently\n",
    "        # ReLU activation adds non-linearity, allowing complex pattern learning\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "\n",
    "        # STEP 4: Add & Normalize (Second Residual Connection)\n",
    "        # out1 + ffn_output: Another residual connection\n",
    "        # layernorm2: Final normalization before passing to next block\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TransformerModel(Model):\n",
    "    \"\"\"\n",
    "    Complete Transformer model for text generation.\n",
    "\n",
    "    ARCHITECTURE:\n",
    "    Input tokens → Embedding → + Positional Encoding → Stack of Transformer Blocks → Dense (vocab prediction)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # DATA PREPARATION - Convert integer tokens to dense vectors\n",
    "        # vocab_size (10000): Number of unique words in vocabulary\n",
    "        # embed_dim (256): Size of embedding vector for each word\n",
    "        # Converts sparse token IDs to dense, learnable representations\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Positional encoding: Gives the model information about token order/position\n",
    "        # Since self-attention has no inherent notion of sequence order,\n",
    "        # we add position information using sine/cosine functions\n",
    "        # Shape: (1, seq_length, embed_dim)\n",
    "        self.pos_encoding = self.positional_encoding(seq_length, embed_dim)\n",
    "\n",
    "        # STACK TRANSFORMER BLOCKS\n",
    "        # num_layers (4): Stack 4 transformer blocks on top of each other\n",
    "        # Each block refines the representation further\n",
    "        # Deeper networks can learn more complex patterns\n",
    "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)]\n",
    "\n",
    "        # FINAL OUTPUT LAYER\n",
    "        # Projects from embed_dim (256) back to vocab_size (10000)\n",
    "        # Outputs logits for each possible next token\n",
    "        self.dense = Dense(vocab_size)\n",
    "\n",
    "    def positional_encoding(self, seq_length, embed_dim):\n",
    "        \"\"\"\n",
    "        Create positional encodings using sine and cosine functions.\n",
    "        Provides information about token position in sequence.\n",
    "        Different frequencies allow model to learn relative positions.\n",
    "        \"\"\"\n",
    "        # Calculate angles for sine/cosine functions\n",
    "        angle_rads = self.get_angles(\n",
    "            np.arange(seq_length)[:, np.newaxis],  # Position indices\n",
    "            np.arange(embed_dim)[np.newaxis, :],    # Dimension indices\n",
    "            embed_dim\n",
    "        )\n",
    "\n",
    "        # Apply sine to even indices, cosine to odd indices\n",
    "        # This creates unique positional signatures for each position\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Even dimensions\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Odd dimensions\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def get_angles(self, pos, i, embed_dim):\n",
    "        \"\"\"\n",
    "        Calculate angle rates for positional encoding.\n",
    "        Different dimensions use different frequencies.\n",
    "        \"\"\"\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
    "        return pos * angle_rates\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass through the complete model.\n",
    "\n",
    "        FLOW:\n",
    "        1. Token IDs → Embeddings (words become vectors)\n",
    "        2. Add positional information (so model knows token order)\n",
    "        3. Pass through stack of transformer blocks (learn relationships and patterns)\n",
    "        4. Project to vocabulary size (predict next token probabilities)\n",
    "\n",
    "        inputs shape: (batch_size, seq_length) - integer token IDs\n",
    "        output shape: (batch_size, seq_length, vocab_size) - logits for each position\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "\n",
    "        # Convert token IDs to embedding vectors\n",
    "        # Shape: (batch_size, seq_length) → (batch_size, seq_length, embed_dim)\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        # Add positional encoding (element-wise addition)\n",
    "        # Now model knows both WHAT each token is (embedding) and WHERE it is (position)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        # Pass through stack of transformer blocks\n",
    "        # Each block: attention → add&norm → FFN → add&norm\n",
    "        # Representations get progressively refined\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "\n",
    "        # Final projection to vocabulary size\n",
    "        # For each position, output scores for all possible next tokens\n",
    "        # Shape: (batch_size, seq_length, vocab_size)\n",
    "        output = self.dense(x)\n",
    "        return output\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:57:18.832867Z",
     "start_time": "2025-10-27T18:57:18.560084Z"
    }
   },
   "source": [
    "# ===== MODEL CONFIGURATION =====\n",
    "\n",
    "# Hyperparameters define model architecture\n",
    "embed_dim = 256    # Size of word embedding vectors (how much info per word)\n",
    "num_heads = 4      # Number of parallel attention heads in each block\n",
    "ff_dim = 512       # Size of hidden layer in feed-forward network\n",
    "num_layers = 4     # Number of transformer blocks to stack (depth of network)\n",
    "\n",
    "# Build the Transformer model\n",
    "# Uses vocab_size (10000) and seq_length (100) from earlier data preparation\n",
    "model = TransformerModel(vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length)\n",
    "\n",
    "# Initialize model weights by passing dummy input\n",
    "# Creates a random batch of token IDs to build all layers\n",
    "# Shape: (batch_size=1, seq_length=100) with values in range [0, vocab_size)\n",
    "_ = model(tf.random.uniform((1, seq_length), maxval=vocab_size, dtype=tf.int32))\n",
    "\n",
    "# Compile the model for training\n",
    "# - optimizer='adam': Adaptive learning rate optimization\n",
    "# - loss='sparse_categorical_crossentropy': For integer target labels (token IDs)\n",
    "#   Compares predicted token probabilities against actual next tokens\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Display model architecture\n",
    "# Shows layers, parameters, and shapes\n",
    "model.summary()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"transformer_model_2\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer_model_2\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001B[38;5;33mEmbedding\u001B[0m)         │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m100\u001B[0m, \u001B[38;5;34m256\u001B[0m)          │     \u001B[38;5;34m2,560,000\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_8             │ ?                      │     \u001B[38;5;34m1,315,840\u001B[0m │\n",
       "│ (\u001B[38;5;33mTransformerBlock\u001B[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_9             │ ?                      │     \u001B[38;5;34m1,315,840\u001B[0m │\n",
       "│ (\u001B[38;5;33mTransformerBlock\u001B[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_10            │ ?                      │     \u001B[38;5;34m1,315,840\u001B[0m │\n",
       "│ (\u001B[38;5;33mTransformerBlock\u001B[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_11            │ ?                      │     \u001B[38;5;34m1,315,840\u001B[0m │\n",
       "│ (\u001B[38;5;33mTransformerBlock\u001B[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m100\u001B[0m, \u001B[38;5;34m10000\u001B[0m)        │     \u001B[38;5;34m2,570,000\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_8             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,840</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_9             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,840</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_10            │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,840</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_11            │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,315,840</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m10,393,360\u001B[0m (39.65 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,393,360</span> (39.65 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m10,393,360\u001B[0m (39.65 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,393,360</span> (39.65 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Train the Transformer model \n",
    "\n",
    "Train the Transformer model on the preprocessed text data. \n",
    "\n",
    "In the following code: \n",
    "\n",
    "- Train the Transformer model on the input and target sequences \n",
    "\n",
    "- Plot the training loss to monitor the model's performance over epochs \n",
    "\n",
    "Model training: \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:17:48.473156Z",
     "start_time": "2025-10-27T18:17:44.143571Z"
    }
   },
   "source": [
    "!pip install matplotlib"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/davidjbrady/venvs/ml_3.11.9_venv/lib/python3.11/site-packages (3.10.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/davidjbrady/venvs/ml_3.11.9_venv/lib/python3.11/site-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/davidjbrady/venvs/ml_3.11.9_venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/davidjbrady/venvs/ml_3.11.9_venv/lib/python3.11/site-packages (from matplotlib) (4.56.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/davidjbrady/venvs/ml_3.11.9_venv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\r\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/davidjbrady/venvs/ml_3.11.9_venv/lib/python3.11/site-packages (from matplotlib) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/davidjbrady/venvs/ml_3.11.9_venv/lib/python3.11/site-packages (from matplotlib) (24.2)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/davidjbrady/venvs/ml_3.11.9_venv/lib/python3.11/site-packages (from matplotlib) (11.1.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/davidjbrady/venvs/ml_3.11.9_venv/lib/python3.11/site-packages (from matplotlib) (3.2.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/davidjbrady/venvs/ml_3.11.9_venv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/davidjbrady/venvs/ml_3.11.9_venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Note: The original dataset is large, we have reduced it to 10,000 samples and limited the training to 2 epochs in order to minimize execution time\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:17:48.494204Z",
     "start_time": "2025-10-27T18:17:48.478165Z"
    }
   },
   "source": [
    "X = X[:10000]\n",
    "Y = Y[:10000]"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:24:25.027593Z",
     "start_time": "2025-10-27T18:17:48.500962Z"
    }
   },
   "source": [
    "# Import necessary libraries for training visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping callback to stop training if the loss doesn't improve\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train the transformer model on the full input and target sequences\n",
    "history = model.fit(X, Y, epochs=2, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Plot training loss to monitor model performance over epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m200s\u001B[0m 629ms/step - loss: 10.6482\n",
      "Epoch 2/2\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m196s\u001B[0m 627ms/step - loss: 10.6226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Generate text with the trained model \n",
    "\n",
    "Define a function to generate text using the trained Transformer model. \n",
    "\n",
    "In the following code: \n",
    "\n",
    "- Define the generate_text function to generate text using the trained Transformer model \n",
    "\n",
    "- Convert the start string into numerical format \n",
    "\n",
    "- Use the model to predict the next word and append it to the generated text \n",
    "\n",
    "- Print the generated text \n",
    "\n",
    "#### Text generation: \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:24:29.222585Z",
     "start_time": "2025-10-27T18:24:25.080420Z"
    }
   },
   "source": [
    "def generate_text(model, start_string, num_generate=100, temperature=1.0):\n",
    "    # Convert the start string to a vectorized format\n",
    "    input_eval = vectorizer([start_string]).numpy()\n",
    "    \n",
    "    # Ensure the input length is the same as the model's expected input shape\n",
    "    if input_eval.shape[1] < seq_length:\n",
    "        # Pad the input if it's shorter than the expected sequence length\n",
    "        padding = np.zeros((1, seq_length - input_eval.shape[1]))\n",
    "        input_eval = np.concatenate((padding, input_eval), axis=1)\n",
    "    elif input_eval.shape[1] > seq_length:\n",
    "        # Truncate the input if it's longer than the expected sequence length\n",
    "        input_eval = input_eval[:, -seq_length:]\n",
    "\n",
    "    input_eval = tf.convert_to_tensor(input_eval)\n",
    "    \n",
    "    # Initialize an empty list to store generated text\n",
    "    text_generated = []\n",
    "\n",
    "    # Start generating text\n",
    "    for i in range(num_generate):\n",
    "        # Make predictions using the model\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        # Remove only the batch dimension, keep the logits as 2D (batch_size, vocab_size)\n",
    "        predictions = predictions[0]  # This should be of shape [vocab_size]\n",
    "\n",
    "        # Apply temperature to predictions\n",
    "        predictions = predictions / temperature\n",
    "        \n",
    "        # Use a categorical distribution to predict the next word\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n",
    "\n",
    "        # Update the input tensor to include the predicted word, maintaining the sequence length\n",
    "        input_eval = np.append(input_eval.numpy(), [[predicted_id]], axis=1)  # Append predicted token\n",
    "        input_eval = input_eval[:, -seq_length:]  # Keep only the last `seq_length` tokens\n",
    "        input_eval = tf.convert_to_tensor(input_eval)  # Convert back to tensor\n",
    "\n",
    "        # Append the predicted word to the generated text\n",
    "        text_generated.append(vectorizer.get_vocabulary()[predicted_id])\n",
    "\n",
    "    # Return the generated text starting from the initial seed\n",
    "    return start_string + ' ' + ' '.join(text_generated)\n",
    "\n",
    "# Generate text with temperature control\n",
    "start_string = \"To be, or not to be\"\n",
    "generated_text = generate_text(model, start_string, temperature=0.7)  # Lower temperature for more focused predictions\n",
    "print(generated_text)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be, or not to be an banishd aufidius lancaster processserver malignant himself answer destroy remedies forth suffolk doubt made unblown doubt seat especially taught mought prelate shal unsuspected coriolanus discretion seconds trucklebed upturned revelld power needed recall forth reformd power prayerbook now due overjoyd hangmen the twelve begging addition bootless rankle readiest alarums aufidius forth holding seamaid infant moisture switch sully daylight mores womankind crew and wherein rushes and plaster sicinius rancorous made packing grant nourish [UNK] hangmen above smeard impartial bedfellow designs privileges much doubt sat loathed sicils corioli sudden forsworn can little can shrouds forth forth tempering hangmen rarest parted himself make infinite\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice exercises \n",
    "\n",
    "> ####  Note: Results may vary since the dataset size was reduced and training was limited to 2 epochs to shorten execution time. However, you are encouraged to experiment with different epoch values for deeper learning\n",
    "\n",
    "### Exercise 1: Experiment with different sequence lengths \n",
    "\n",
    "**Objective:** Implement different sequence lengths to understand their effect on the performance of the Transformer model. \n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Change the sequence length to 50 \n",
    "\n",
    "- Preprocess the data set with the new sequence length \n",
    "\n",
    "- Train the model and compare the training loss \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:24:29.227895Z",
     "start_time": "2025-10-27T18:24:29.226295Z"
    }
   },
   "source": [
    "# Write your code here"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the dataset \n",
    "vocab_size = 10000 \n",
    "seq_length = 50 \n",
    "\n",
    "# Adapt TextVectorization to full text \n",
    "vectorizer = TextVectorization(max_tokens=vocab_size, output_mode='int') \n",
    "text_ds = tf.data.Dataset.from_tensor_slices([text]).batch(1) \n",
    "vectorizer.adapt(text_ds) \n",
    "\n",
    "# Vectorize the text \n",
    "vectorized_text = vectorizer([text])[0] \n",
    "print(\"Vectorized text shape:\", vectorized_text.shape) \n",
    "print(\"First 10 vectorized tokens:\", vectorized_text.numpy()[:10]) \n",
    "\n",
    "X, Y = create_sequences(vectorized_text.numpy(), seq_length) \n",
    "\n",
    "\n",
    "# Check if sequences are correctly generated \n",
    "print(\"Number of sequences generated:\", len(X)) \n",
    "print(\"Sample input sequence:\", X[0] if len(X) > 0 else \"No sequences generated\") \n",
    "\n",
    "# Check if X and Y are not empty \n",
    "assert X.size > 0, \"Input data X is empty\" \n",
    "assert Y.size > 0, \"Target data Y is empty\" \n",
    "X = tf.convert_to_tensor(X) \n",
    "Y = tf.convert_to_tensor(Y) \n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape)\n",
    "X = X[:10000]\n",
    "Y = Y[:10000]\n",
    "# Hyperparameters \n",
    "embed_dim = 256 \n",
    "num_heads = 4 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Build the Transformer model \n",
    "model = TransformerModel(vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length)\n",
    "\n",
    "# Provide input shape to build the model by passing a dummy input with maxval specified\n",
    "_ = model(tf.random.uniform((1, seq_length), maxval=vocab_size, dtype=tf.int32))\n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Summary of the model \n",
    "model.summary()\n",
    "# Early stopping callback to stop training if the loss doesn't improve\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train the transformer model on the full input and target sequences\n",
    "history = model.fit(X, Y, epochs=2, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Plot training loss to monitor model performance over epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Add a learning rate scheduler \n",
    "\n",
    "**Objective:** Implement a learning rate scheduler to adjust the learning rate during training. \n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Define a learning rate scheduler that reduces the learning rate by half every 10 epochs \n",
    "\n",
    "- Train the model with the learning rate scheduler and compare the training loss \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:24:29.236432Z",
     "start_time": "2025-10-27T18:24:29.235034Z"
    }
   },
   "source": [
    "# Write your code here"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Write your code here\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a learning rate scheduler  \n",
    "def scheduler(epoch, lr):  \n",
    "    if epoch % 10 == 0 and epoch != 0:  \n",
    "        lr = lr * 0.5  \n",
    "    return lr  \n",
    " \n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)  \n",
    "\n",
    "\n",
    "# Train the model with the learning rate scheduler  \n",
    "history = model.fit(X, Y, epochs=2, batch_size=64, callbacks=[callback])  \n",
    "   \n",
    "\n",
    "# Plot the training loss  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.xlabel('Epoch')  \n",
    "plt.ylabel('Loss')  \n",
    "plt.title('Training Loss with Learning Rate Scheduler')  \n",
    "plt.show() \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Generate longer text sequences \n",
    "\n",
    "**Objective:** To explore the model's text generation capabilities and generate longer sequences. \n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Modify the `generate_text` function to generate 200 tokens instead of 100 \n",
    "\n",
    "- Generate text using the trained model and the modified function \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T18:24:29.242062Z",
     "start_time": "2025-10-27T18:24:29.240834Z"
    }
   },
   "source": [
    "# Write your code here"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_text(model, start_string, num_generate=200):\n",
    "    # Convert the start string to numbers (vectorize)\n",
    "    input_eval = vectorizer([start_string]).numpy()\n",
    "\n",
    "    # Ensure the input tensor has the correct shape\n",
    "    input_eval = tf.convert_to_tensor(input_eval[:, -5:])  # Ensure it has a shape of (1, 5)\n",
    "    \n",
    "    text_generated = []\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        # Make predictions using the model\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        # Ensure predictions is a matrix with shape [batch_size, num_classes]\n",
    "        predictions = tf.squeeze(predictions, 0)  # Remove the batch dimension\n",
    "        predictions = tf.expand_dims(predictions, 0)  # Add back a batch dimension for categorical\n",
    "        \n",
    "        # Use a categorical distribution to predict the next word\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Update the input tensor to include the predicted word, maintaining the sequence length\n",
    "        input_eval = np.append(input_eval.numpy(), [[predicted_id]], axis=1)  # Append predicted token\n",
    "        input_eval = input_eval[:, -5:]  # Keep only the last 5 tokens to match input shape\n",
    "        input_eval = tf.convert_to_tensor(input_eval)  # Convert back to tensor\n",
    "        \n",
    "        # Add the predicted word to the generated text\n",
    "        text_generated.append(vectorizer.get_vocabulary()[predicted_id])\n",
    "\n",
    "    return start_string + ' ' + ' '.join(text_generated)\n",
    "\n",
    "\n",
    "# Generate longer text\n",
    "start_string = \"To be, or not to be\"\n",
    "generated_text = generate_text(model, start_string)\n",
    "\n",
    "print(generated_text)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion \n",
    "\n",
    "Congratulations on completing this lab! You have successfully built and trained a Transformer model for text generation in this lab using TensorFlow and Keras. You learned how to preprocess text data, create input and target sequences, define the Transformer model architecture, train the model, and generate text using the trained model. By completing this lab, you gained hands-on experience with Transformers for text generation and explored practical applications of this robust model architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "fffe2e8a6841414ac70c57a6272bf08575362563ed7123ff0baf7c2db7be259e"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
