{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with PyTorch\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "As a machine learning engineer hired by a social media company, your task is determining whether songs shared on the platform are appropriate for children. Given the high costs associated with processing each song using large language models (LLMs) for content evaluation, an alternative method using retrieval-augmented generation (RAG) is proposed. RAG combines the benefits of a retriever model, which fetches relevant information (in this case, embeddings of pre-answered content appropriateness questions), and a generator model, which uses this information to predict the appropriateness of new content. This approach efficiently scales the evaluation process while ensuring that each song's content is scrutinized for child safety without the overhead of running a full LLM for each song.\n",
    "\n",
    "![A big yellow bird puppet and a purple dinosaur puppet singing next to a rapper and a metal band singer](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/MGy9JGkmyNnnTPqNlXk2uw/The%20scene%20is%20lively%20and%20colorful.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "            <li><a href=\"#Defining-helper-functions\">Defining helper functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Embeddings-using-BERT-and-PyTorch-Hub\">Embeddings using BERT and PyTorch Hub</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Loading-tokenizer-and-model\">Loading tokenizer and model</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Tokenization\">Tokenization</a></li>\n",
    "                    <li><a href=\"#Text-decoding-and-verification\">Text decoding and verification</a></li>\n",
    "                    <li><a href=\"#Device-and-Convert-Tensor\">Device and Convert Tensor</a></li>\n",
    "                    <li><a href=\"#Loading-the-BERT-model\">Loading the BERT model</a></li>\n",
    "                    <li><a href=\"#Generating-aggregated-mean-embeddings-using-BERT-for-RAG\">Generating aggregated mean embeddings using BERT for RAG</a></li>\n",
    "                </ol>\n",
    "            </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Convert-questions-and-the-song-lyrics-into-embeddings\">Convert questions and the song lyrics into embeddings</a></li>\n",
    "    <li><a href=\"#Understanding-the-dot-product\">Understanding the dot product</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Exercise\">Exercise</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Task-description\">Task description</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- **Understand embedding techniques**: Learn how to generate and use embeddings from pre-trained models for natural language processing tasks.\n",
    "- **Use Hugging Face Transformers with PyTorch**: Apply the transformers library to load and work with advanced NLP models like BERT in PyTorch..\n",
    "- **Apply t-SNE for visualization**: Use t-SNE to visualize high-dimensional data in lower-dimensional spaces, enhancing understanding of data distribution and clustering.\n",
    "- **Fine-tuning language models**: Gain practical experience in fine-tuning pretrained language models for specific tasks, enhancing model performance on targeted datasets.\n",
    "- **Develop practical NLP solutions**: Implement a system that effectively uses Retriever and Generator architectures to answer queries, demonstrating an end-to-end workflow from model tuning to deployment.\n",
    "- **Implement cosine similarity**: Replace dot product similarity measures with cosine similarity to improve relevance detection in response generation systems.\n",
    "- **Evaluate model performance**: Assess the impact of using cosine similarity over dot product in terms of retrieval accuracy and relevance in a QA system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you will be using the following libraries. These libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n",
    "\n",
    "***Note : After installing please ensure that you restart the kernel and execute the subsequent cells.***\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n%pip install --user transformers==4.40.2 numpy sacremoses==0.1.1 matplotlib==3.8.4 sentencepiece==0.2.0 scikit-learn==1.4.2\n%pip install torch==2.8.0+cpu torchtext==0.18.0+cpu  \\\n    --index-url https://download.pytorch.org/whl/cpu\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:00:53.482626Z",
     "start_time": "2025-12-19T22:00:53.480371Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "import warnings\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining helper functions\n",
    "\n",
    "The `tsne_plot` function applies t-SNE to reduce high-dimensional data to three dimensions and creates a 3D scatter plot of the results. It sets the `perplexity` to the number of data points minus one, plots each point in a unique color based on its index, and labels the axes for each t-SNE component. The visualization helps in exploring patterns in a 3D space.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:00:56.378662Z",
     "start_time": "2025-12-19T22:00:56.375461Z"
    }
   },
   "source": [
    "def tsne_plot(data, plot):\n    # Apply t-SNE to reduce to 3D\n    tsne = TSNE(n_components=3, random_state=42, perplexity=min(50, data.shape[0] - 1))  # Using 50 or less based on data size\n    data_3d = tsne.fit_transform(data)\n    \n    # Plotting\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    # Assign colors for each point based on its index\n    colors = plt.cm.rainbow(np.linspace(0, 1, len(data_3d)))\n    for idx, point in zip(range(len(data_3d)), data_3d):\n        ax.scatter(point[0], point[1], point[2], color=colors[idx], label=f'{plot} {idx+1}')\n    \n    # Adding labels and titles\n    ax.set_xlabel('TSNE Component 1')\n    ax.set_ylabel('TSNE Component 2')\n    ax.set_zlabel('TSNE Component 3')\n    plt.title('3D t-SNE Visualization of '+ plot +' Embeddings')\n    plt.legend(title=plot +' Index', bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.show()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings using BERT and PyTorch Hub\n",
    "\n",
    "Use PyTorch and the Transformers library by Hugging Face to tokenize text, convert it to embeddings using BERT, and handle these embeddings with a model.\n",
    "\n",
    "## Loading tokenizer and model \n",
    "Let's begin by loading a tokenizer and later a model, both specifically bert-base-uncased. This is done using torch.hub.load, which is a convenient way to load pre-trained models and tokenizers directly from Hugging Face's model hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:01:30.932909Z",
     "start_time": "2025-12-19T22:01:07.815733Z"
    }
   },
   "source": [
    "from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input_text variable is set as a list of tuples. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:01:38.355279Z",
     "start_time": "2025-12-19T22:01:38.353619Z"
    }
   },
   "source": [
    "# Input text to get embeddings for\ninput_text = [(\"This is an example sentence for BERT embeddings.\", \"How do you like it \"),(\"There are other models\")]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "```batch_encode_plus``` method is used for tokenizing text. It automatically handles padding and truncation to ensure uniformity in input length, which is crucial for batch processing in models like BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:01:40.967943Z",
     "start_time": "2025-12-19T22:01:40.963649Z"
    }
   },
   "source": [
    "input_ids = tokenizer.batch_encode_plus(input_text,add_special_tokens=True,padding=True,truncation=True)\ninput_ids"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2023, 2003, 2019, 2742, 6251, 2005, 14324, 7861, 8270, 4667, 2015, 1012, 102, 2129, 2079, 2017, 2066, 2009, 102], [101, 2045, 2024, 2060, 4275, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text decoding and verification\n",
    "After tokenization, the script decodes the first tokenized input back to text to check the correctness and length of the tokenization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:02:18.803959Z",
     "start_time": "2025-12-19T22:01:44.254763Z"
    }
   },
   "source": [
    "text=tokenizer.decode(input_ids['input_ids'][0])\nprint(text)\nprint(f\"length {len(text.split())}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] this is an example sentence for bert embeddings. [SEP] how do you like it [SEP]\n",
      "length 16\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:02:32.105754Z",
     "start_time": "2025-12-19T22:02:32.102829Z"
    }
   },
   "source": [
    "input_ids['attention_mask']"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When using the BERT tokenizer, the output includes key components that are essential for the model's processing:\n",
    "\n",
    "```input_ids```: A list of token IDs that represent each token in BERT's vocabulary.\n",
    "\n",
    "```token_type_ids```: Indicates which sentence each token belongs to, important for tasks involving sentence pairs.\n",
    "\n",
    "```attention_mask```: Identifies which tokens should be focused on, differentiating real content from padding.\n",
    "\n",
    "\n",
    "Special tokens:\n",
    "\n",
    "[CLS]: Placed at the start of every input for use in classification tasks.\n",
    "\n",
    "[SEP]: Separates sentences in dual-sentence tasks and marks the end of input sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device and Convert Tensor\n",
    "Here, you are going to convert the token IDs and attention masks into PyTorch tensors and transfers them to a computing device (DEVICE) for input into BERT. This device should be defined elsewhere in the script (typically as \"cuda\" for GPU or \"cpu\").\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:03:51.493184Z",
     "start_time": "2025-12-19T22:03:51.490339Z"
    }
   },
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDEVICE"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:03:53.720454Z",
     "start_time": "2025-12-19T22:03:53.717862Z"
    }
   },
   "source": [
    "input_ids_tensors = torch.tensor(input_ids['input_ids']).to(DEVICE)\nmask_tensors = torch.tensor(input_ids['attention_mask']).to(DEVICE)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the BERT model\n",
    "Now, load the BERT model and move it to the same device as the input tensors. The model then processes these inputs to generate embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:04:36.402461Z",
     "start_time": "2025-12-19T22:03:57.196858Z"
    }
   },
   "source": [
    "from transformers import BertModel\nbert_model = BertModel.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c87b7b1003b40b8a18acab3fc2b14fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:04:44.888830Z",
     "start_time": "2025-12-19T22:04:44.883883Z"
    }
   },
   "source": [
    "bert_model.to(DEVICE)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:04:52.802716Z",
     "start_time": "2025-12-19T22:04:52.467747Z"
    }
   },
   "source": [
    "word_embding=bert_model(input_ids_tensors,mask_tensors)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating aggregated mean embeddings using BERT for RAG\n",
    "Here, you'll compute aggregated mean embeddings for input sequences using the BERT model you just loaded. It processes each pair of token IDs and attention masks from the input data, extracts word embeddings for non-padded tokens, and calculates their mean. The result is a list of mean embeddings for each sequence, which is then concatenated into a single tensor. This process allows for the generation of simplified yet informative representations of the input sequences, useful for tasks like clustering, similarity search, or as input to downstream models. Each document must be under 512 tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T22:04:57.585420Z",
     "start_time": "2025-12-19T22:04:57.502343Z"
    }
   },
   "source": [
    "# Initialize a list to store the mean embeddings for each input sequence\naggregated_mean_embeddings = []\n\n# Loop over each pair of input_ids and attention_masks\nfor token_ids, attention_mask in tqdm(zip(input_ids['input_ids'], input_ids['attention_mask'])):\n    # Convert list of token ids and attention mask to tensors\n    token_ids_tensor = torch.tensor([token_ids]).to(DEVICE)\n    attention_mask_tensor = torch.tensor([attention_mask]).to(DEVICE)\n    print(\"token_ids_tensor shape:\",token_ids_tensor.shape, attention_mask_tensor.shape)  # Print the shapes of the input tensors\n    with torch.no_grad():  # Disable gradient calculations for faster execution\n        # Retrieve the batch of word embeddings from the BERT model\n        embeddings = bert_model(token_ids_tensor, attention_mask=attention_mask_tensor)[0].squeeze(0)\n        print(\"Word embeddings shape:\", embeddings.shape)\n        \n        # Count and print the number of zero-padding embeddings\n        num_zero_paddings = (attention_mask_tensor == 0).sum().item()\n        print(\"Number of zero padding embeddings:\", num_zero_paddings)\n        \n        # Create a mask for positions that are not zero-padded\n        valid_embeddings_mask = attention_mask_tensor[0] != 0\n        print(\"valid_embeddings_mask:\",valid_embeddings_mask)\n        \n        # Filter out the embeddings corresponding to zero-padded positions\n        filtered_embeddings = embeddings[valid_embeddings_mask, :]\n        print(\"Word embeddings after zero padding embeddings removed:\", filtered_embeddings.shape)\n        \n        # Compute the mean of the filtered embeddings\n        mean_embedding = filtered_embeddings.mean(axis=0)\n        print(\"Mean embedding shape:\", mean_embedding.shape)\n    \n        # Append the mean embedding to the list, adding a batch dimension\n        aggregated_mean_embeddings.append(mean_embedding.unsqueeze(0))\n\n# Concatenate all mean embeddings to form a single tensor\naggregated_mean_embeddings = torch.cat(aggregated_mean_embeddings)\nprint('All mean embeddings shape:', aggregated_mean_embeddings.shape)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 25.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids_tensor shape: torch.Size([1, 20]) torch.Size([1, 20])\n",
      "Word embeddings shape: torch.Size([20, 768])\n",
      "Number of zero padding embeddings: 0\n",
      "valid_embeddings_mask: tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True])\n",
      "Word embeddings after zero padding embeddings removed: torch.Size([20, 768])\n",
      "Mean embedding shape: torch.Size([768])\n",
      "token_ids_tensor shape: torch.Size([1, 20]) torch.Size([1, 20])\n",
      "Word embeddings shape: torch.Size([20, 768])\n",
      "Number of zero padding embeddings: 14\n",
      "valid_embeddings_mask: tensor([ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "Word embeddings after zero padding embeddings removed: torch.Size([6, 768])\n",
      "Mean embedding shape: torch.Size([768])\n",
      "All mean embeddings shape: torch.Size([2, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is converted to the function ```aggregate_embeddings``` that takes token indices and their corresponding attention masks, and uses a BERT model to convert these tokens into word embeddings. It then filters out the embeddings for zero-padded tokens and computes the mean embedding for each sequence. This helps in reducing the dimensionality of the data while retaining the most important information from the embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_embeddings(input_ids, attention_masks, bert_model=bert_model):\n    \"\"\"\n    Converts token indices and masks to word embeddings, filters out zero-padded embeddings,\n    and aggregates them by computing the mean embedding for each input sequence.\n\n    \"\"\"\n    mean_embeddings = []\n    # Process each sequence in the batch\n    print('number of inputs',len(input_ids))\n    for input_id, mask in tqdm(zip(input_ids, attention_masks)):\n        input_ids_tensor = torch.tensor([input_id]).to(DEVICE)\n        mask_tensor = torch.tensor([mask]).to(DEVICE)\n\n        with torch.no_grad():\n            # Obtain the word embeddings from the BERT model\n            word_embeddings = bert_model(input_ids_tensor, attention_mask=mask_tensor)[0].squeeze(0)\n\n            # Filter out the embeddings at positions where the mask is zero \n            valid_embeddings_mask=mask_tensor[0] != 0 \n            valid_embeddings = word_embeddings[valid_embeddings_mask,:]\n            # Compute the mean of the filtered embeddings\n            mean_embedding = valid_embeddings.mean(dim=0)\n            mean_embeddings.append(mean_embedding.unsqueeze(0))\n\n    # Concatenate the mean embeddings from all sequences in the batch\n    aggregated_mean_embeddings = torch.cat(mean_embeddings)\n    return aggregated_mean_embeddings"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further simplify the process with the function ```text_to_emb``` which is designed to convert a list of text strings into their corresponding embeddings using a pre-defined tokenizer and calling ```aggregate_embeddings```.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_emb(list_of_text,max_input=512):\n    data_token_index  = tokenizer.batch_encode_plus(list_of_text, add_special_tokens=True,padding=True,truncation=True,max_length=max_input)\n    question_embeddings=aggregate_embeddings(data_token_index['input_ids'], data_token_index['attention_mask'])\n    return question_embeddings"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert questions and the song lyrics into embeddings\n",
    "\n",
    "Let's first convert both the predefined appropriateness questions and the song lyrics into \"RAG embeddings\". Then, you measure the similarity between these embeddings to determine appropriateness:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\ndef process_song(song):\n    # Remove line breaks from the song\n    song_new = re.sub(r'[\\n]', ' ', song)\n    \n    # Remove single quotes from the song\n    song_new = [song_new.replace(\"\\'\", \"\")]\n    \n    return song_new"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find more such sample questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_questions = [\n    \"Does this song contain any violent themes, such as references to guns, killing, or physical aggression? Example: Does the song describe or promote physical violence, like fighting or shootings?\",\n    \"Are there any explicit lyrics or bad words used in this song that might be considered offensive or inappropriate? Example: Does the song use language commonly recognized as profanity or derogatory terms?\",\n    \"Is the overall content of this song suitable for children, considering its themes, language, and messages? Example: Are there elements in the song that could be deemed too mature or unsuitable for young listeners?\",\n    \"Does this song explicitly mention weapons, such as guns, knives, or other similar items? Example: Are specific types of weapons described or glorified in the lyrics?\",\n    \"Are the messages conveyed in this song positive and uplifting for children? Example: Does the song promote values like kindness, friendship, and positivity?\",\n    \"Does this song include any sexual content, references to sexual behavior, or suggestive language? Example: Are there lyrics that explicitly or implicitly discuss sexual themes or experiences?\",\n    \"Does this song offer any educational value, such as teaching the alphabet, basic math, or other learning content? Example: Are there educational segments in the song that could help children learn fundamental skills like the ABCs or counting?\",\n    \"Does this song promote emotional resilience and social skills among children? Example: Does the song include themes of overcoming challenges or building friendships?\"\n]"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the questions to embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_questions=street=text_to_emb(song_questions)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the question embeddings through a t-SNE plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(embeddings_questions, \"Question\")"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also develop a set of responses when a song has similar attributes. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_responses = [\n    \"Yes, this song contains violent themes, including references to guns, killing, or physical aggression, and is not suitable for children.\",\n    \"Yes, this song includes explicit lyrics or bad words that might be considered offensive or inappropriate for young audiences.\",\n    \"No, the overall content of this song is not suitable for children as it includes themes, language, and messages that are too mature or unsuitable for young listeners.\",\n    \"Yes, this song explicitly mentions weapons, such as guns and knives, which could be disturbing or inappropriate for children’s entertainment.\",\n    \"Yes, the messages conveyed in this song are positive and uplifting, promoting values like kindness, friendship, and positivity, beneficial for children.\",\n    \"Yes, this song includes sexual content and references to sexual behavior or suggestive language, which are inappropriate for a child-friendly environment.\",\n    \"Yes, this song offers significant educational value, including segments that teach the alphabet, basic math, and other learning content, making it both fun and educational for children.\",\n    \"Yes, this song promotes emotional resilience and social skills, incorporating themes about overcoming challenges and building friendships, which are essential for children's development.\"\n]"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the responses to their embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_responses = text_to_emb(yes_responses)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the responses embeddings through a t-SNE plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(embeddings_responses, \"Response\")"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the song [Bullet in the Head](https://en.wikipedia.org/wiki/Bullet_in_the_Head_(song)) by the activist group 'Rage Against the Machine'. The band portrays the government's use of media as a tool for population control, drawing stark comparisons between ordinary residents and Alcatraz. This song is clearly geared towards adults and is not appropriate for children due to the explicit language and violent rhetoric, which could be confusing and unsuitable for younger audiences.\n",
    "\n",
    "<span style=\"color:red\">Note: To browse the lyrics you can go to [Bullet in the Head](https://www.google.com/search?q=bullet+in+your+head+lyrics&rlz=1C5GCCM_en&oq=bulet+in+you+head+l&gs_lcrp=EgZjaHJvbWUqCQgBEAAYDRiABDIGCAAQRRg5MgkIARAAGA0YgAQyCQgCEC4YDRiABDIJCAMQABgNGIAEMggIBBAAGBYYHjIICAUQABgWGB4yCAgGEAAYFhgeMggIBxAAGBYYHjIICAgQABgWGB4yCAgJEAAYFhge0gEINjM1NGowajeoAgCwAgA&sourceid=chrome&ie=UTF-8) and then paste the content of the song in below cell and process it as other songs. Also, note that the song lyrics is extremely abusive</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_rage = \"\"\"\n\"\"\""
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some minor preprocessing of the text, you can generate the RAG embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_rage=process_song(song_rage)\nembeddings_rage=text_to_emb(song_rage)\nembeddings_rage.shape"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the song [Can You Tell Me How to Get to Sesame Street?](https://en.wikipedia.org/wiki/Can_You_Tell_Me_How_to_Get_to_Sesame_Street%3F) — the introductory theme song of a popular children's show. Similar to previous methods, the text is processed to generate RAG embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "sesame_street = \"\"\"\nSunny day\nSweepin' the clouds away\nOn my way to where the air is sweet\nCan you tell me how to get\nHow to get to Sesame Street?\n\nCome and play\nEverything's A-okay\nFriendly neighbors there\nThat's where we meet\nCan you tell me how to get\nHow to get to Sesame Street?\n\nIt's a magic carpet ride\nEvery door will open wide\nTo happy people like you\nHappy people like\nWhat a beautiful\n\nSunny day\nSweepin' the clouds away\nOn my way to where the air is sweet\nCan you tell me how to get\nHow to get to Sesame Street?\nHow to get to Sesame Street?\nHow to get to Sesame Street?\nHow to get to Sesame Street?\nHow to get to Sesame Street?\n\"\"\""
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_sesame_street= process_song(sesame_street)\nembeddings_sesame_street=text_to_emb(song_sesame_street)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you have the song [Straight Outta Compton](https://en.wikipedia.org/wiki/Straight_Outta_Compton) — a track known for its intense themes, which nearly led to it being banned in the U.S. As with the previous examples, process the lyrics to calculate the word embeddings for a detailed analysis of its content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Note: To browse the lyrics you can go to [Straight Outta Compton Lyrics](https://www.google.com/search?q=straight+outta+compton+song+lyrics&sca_esv=4f737f0b4de0cae7&sca_upv=1&rlz=1C5GCEM_enUS1092CA1092&sxsrf=ADLYWIIPASIwexxpdt-VqsvSF759A-iolw%3A1724787162733&ei=2inOZsW0LNSp5NoPq_XR6Qw&ved=0ahUKEwjFiZfx9JWIAxXUFFkFHat6NM0Q4dUDCA8&uact=5&oq=straight+outta+compton+song+lyrics&gs_lp=Egxnd3Mtd2l6LXNlcnAiInN0cmFpZ2h0IG91dHRhIGNvbXB0b24gc29uZyBseXJpY3MyCxAAGIAEGJECGIoFMgYQABgWGB4yBhAAGBYYHjIGEAAYFhgeMgsQABiABBiGAxiKBTILEAAYgAQYhgMYigUyCxAAGIAEGIYDGIoFMggQABiABBiiBDIIEAAYgAQYogQyCBAAGKIEGIkFSOQjUPEEWKghcAF4AZABAJgBqQGgAeAKqgEDNi42uAEDyAEA-AEBmAINoAKEC8ICChAAGLADGNYEGEfCAg0QABiABBiwAxhDGIoFwgIOEAAYsAMY5AIY1gTYAQHCAhMQLhiABBiwAxhDGMgDGIoF2AECwgIFEAAYgATCAg0QLhiABBixAxhDGIoFwgIKEAAYgAQYQxiKBcICChAuGIAEGEMYigXCAgUQLhiABMICHRAuGIAEGJcFGNwEGN4EGOAEGPQDGPEDGPUD2AEDwgIIEAAYFhgKGB6YAwCIBgGQBhK6BgYIARABGAm6BgYIAhABGAi6BgYIAxABGBSSBwM1LjigB7GKAQ&sclient=gws-wiz-serp) and then paste the content of the song in below cell and process it as other songs. Also, note that the song lyrics is extremely abusive</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "straight_outta_compton_lyrics = \"\"\"\n\"\"\""
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "straight_outta_compton_lyrics= process_song(straight_outta_compton_lyrics)\nembeddings_compton=text_to_emb(straight_outta_compton_lyrics)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider one more song.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_shoe_lyrics=\"\"\"Barney is a dinosaur from our imagination\nAnd when he's tall\nHe's what we call a dinosaur sensation\nBarney's friends are big and small\nThey come from lots of places\nAfter school they meet to play\nAnd sing with happy faces\nBarney shows us lots of things\nLike how to play pretend\nABC's, and 123's\nAnd how to be a friend\nBarney comes to play with us\nWhenever we may need him\nBarney can be your friend too\nIf you just make-believe him!\"\"\""
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_shoe_lyrics= process_song(my_shoe_lyrics)\nembeddings_my_shoe=text_to_emb(my_shoe_lyrics)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a single array from the embeddings of the songs you have till now and plot it through the tsne_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = [song_rage, song_sesame_street, straight_outta_compton_lyrics, my_shoe_lyrics]\nembeddings = [text_to_emb(song) for song in songs]\nall_embeddings = np.vstack(embeddings)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(all_embeddings, \"Song\")"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the dot product\n",
    "\n",
    "The dot product of two vectors, $\\mathbf{a}$ and $\\mathbf{b}$, each with components $a_i$ and $b_i$, is a fundamental operation in linear algebra calculated as follows:\n",
    "\n",
    "$\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i$\n",
    "\n",
    "This calculation results in a single scalar value. Geometrically, the dot product quantifies how much one vector projects onto another or how close they are to eachother.\n",
    "\n",
    "Now, consider the matrix $A$, where each row is the RAG embeddings. In the context of matrix multiplication involving a dot product, if $A$ is a matrix and $\\mathbf{b}$ is a vector, then $A\\mathbf{b}$ computes the dot product of $\\mathbf{b}$ with each row of $A$. In this scenario, each vector in $A$ is treated as a row vector. Here, $A$ represents `embeddings_questions`, while `embeddings_rage` is the RAG vector $\\mathbf{b}$. The output is a column tensor that measures how closely each RAG vector for the questions aligns with the song.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product = embeddings_questions@embeddings_rage.T"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the tensor for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product = dot_product.reshape(-1)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the questions that are closest to the song by sorting them and converting the result to a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = torch.argsort(dot_product,descending=True)\nsorted_indices=sorted_indices.tolist()"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nearest vectors corresponding to the best response can be printed in the following manner. For simplicity, use the topmost response using ``yes_responses``.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "for indices in  sorted_indices[0:3]:\n    print(yes_responses[indices])"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, convert the above to a function where you only input the embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAG_QA(embeddings_questions, embeddings, n_responses=3):\n    # Calculate the dot product between the question embeddings and the provided embeddings (transpose of the second matrix for proper alignment).\n    dot_product = embeddings_questions @ embeddings.T\n\n    # Reshape the dot product results to a 1D tensor for easier processing.\n    dot_product = dot_product.reshape(-1)\n\n    # Sort the indices of the dot product results in descending order (setting descending to False should be True for typical similarity tasks).\n    sorted_indices = torch.argsort(dot_product, descending=True)\n\n    # Convert sorted indices to a list for easier iteration.\n    sorted_indices = sorted_indices.tolist()\n\n    # Print the top 'n_responses' responses from the sorted list, which correspond to the highest dot product values.\n    for index in sorted_indices[:n_responses]:\n        print(yes_responses[index])"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the ```RAG_QA``` for the songs defined above to see the top three responses by RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_QA(embeddings_questions, embeddings_sesame_street)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_QA(embeddings_questions, embeddings_compton)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_QA(embeddings_questions, embeddings_my_shoe)"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "In many machine learning and natural language processing tasks, measuring the similarity between vectors is crucial. While the dot product is a common measure, it considers both the magnitude and direction of the vectors. In contrast, **cosine similarity** measures the cosine of the angle between two vectors, providing a similarity value that purely reflects their orientation in space regardless of their magnitude. This makes cosine similarity particularly useful in text processing where only the directionality of the vectors (i.e., the orientation of the words in the vector space) matters, not their length.\n",
    "\n",
    "Your task is to modify the `RAG_QA()` function, which currently uses the dot product to find the most relevant responses to a given query. You will replace the dot product with cosine similarity to improve the function's ability to determine response relevance based solely on the direction of the vectors.\n",
    "\n",
    "### Task description\n",
    "1. **Modify the function**: Replace the dot product calculation in the `RAG_QA()` function with cosine similarity. Remember, cosine similarity is defined as:\n",
    "\n",
    "   $\n",
    "   \\text{Cosine Similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\n",
    "   $\n",
    "\n",
    "   where:\n",
    "   - $\\mathbf{A} \\cdot \\mathbf{B}$ is the dot product of vectors $\\mathbf{A}$ and $\\mathbf{B}$,\n",
    "   - $\\|\\mathbf{A}\\|$ and $\\|\\mathbf{B}\\|$ are the norms (or magnitudes) of vectors $\\mathbf{A}$ and $\\mathbf{B}$ respectively.\n",
    "\n",
    "2. **Calculate norms**: Compute the norms of the question embeddings and the response embeddings.\n",
    "\n",
    "3. **Compute cosine similarity**: Adjust the computation of similarity by dividing the dot product by the product of the norms of the question and response embeddings.\n",
    "\n",
    "4. **Sort and select responses**: After computing the cosine similarities, sort the responses based on these values and select the top responses as done currently with the dot product.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "def RAG_QA_cosine(embeddings, n_responses=3):\n",
    "    # Calculate the magnitudes (norms) of the question and response embeddings\n",
    "    question_norms = torch.norm(embeddings_questions, dim=1, keepdim=True)\n",
    "    response_norms = torch.norm(embeddings, dim=1, keepdim=True)\n",
    "    \n",
    "    # Calculate the dot product between the question embeddings and the provided embeddings (transpose of the second matrix for proper alignment)\n",
    "    dot_product = torch.mm(embeddings_questions, embeddings.T)\n",
    "    \n",
    "    # Calculate cosine similarity by dividing the dot product by the product of the magnitudes\n",
    "    cosine_similarity = dot_product / (question_norms * response_norms.T)\n",
    "    \n",
    "    # Flatten the cosine similarity tensor to a 1D tensor for easier processing\n",
    "    cosine_similarity = cosine_similarity.reshape(-1)\n",
    "    \n",
    "    # Sort the indices of the cosine similarity results in descending order to get the indices with the highest similarity\n",
    "    sorted_indices = torch.argsort(cosine_similarity, descending=True)\n",
    "    \n",
    "    # Convert sorted indices to a list for easier iteration\n",
    "    sorted_indices = sorted_indices.tolist()\n",
    "    \n",
    "    # Print the top 'n_responses' responses from the sorted list, which correspond to the highest cosine similarity values\n",
    "    for index in sorted_indices[:n_responses]:\n",
    "        print(yes_responses[index])  # Ensure 'responses' is defined and accessible in your scope\n",
    "\n",
    "RAG_QA_cosine(embeddings_my_shoe, n_responses=3)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo) has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his Ph.D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ashutosh Sagar](https://www.linkedin.com/in/ashutoshsagar/) is completing his MS in CS from Dalhousie University. He has previous experience working with Natural Language Processing and as a Data Scientist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "a03e6b3e7ef2f936a6e064db6744e57e3b1894b888a9856a7f91331ee0035c9f"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
