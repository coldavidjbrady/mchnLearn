{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork/images/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Artificial Neural Networks - Forward Propagation</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will build a neural network from scratch and code how it performs predictions using forward propagation. Please note that all deep learning libraries have the entire training and prediction processes implemented, and so in practice you wouldn't really need to build a neural network from scratch. However, hopefully completing this lab will help you understand neural networks and how they work even better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Artificial Neural Networks - Forward Propagation</h2>\n",
    "\n",
    "<h3>Objective for this Notebook<h3>    \n",
    "<h5> 1. Initalize a Network</h5>\n",
    "<h5> 2. Compute Weighted Sum at Each Node. </h5>\n",
    "<h5> 3. Compute Node Activation </h5>\n",
    "<h5> 4. Access your <b>Flask</b> app via a webpage anywhere using a custom link. </h5>     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>    \n",
    "\n",
    "1. <a href=\"#item11\">Recap</a>\n",
    "2. <a href=\"#item12\">Initalize a Network</a>  \n",
    "3. <a href=\"#item13\">Compute Weighted Sum at Each Node</a>  \n",
    "4. <a href=\"#item14\">Compute Node Activation</a>  \n",
    "5. <a href=\"#item15\">Forward Propagation</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"item1\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item11'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the videos, let's recap how a neural network makes predictions through the forward propagation process. Here is a neural network that takes two inputs, has one hidden layer with two nodes, and an output layer with one node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cocl.us/neural_network_example\" alt=\"Neural Network Example\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by randomly initializing the weights and the biases in the network. We have 6 weights and 3 biases, one for each node in the hidden layer as well as for each node in the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:18.152363Z",
     "start_time": "2025-02-23T06:17:18.150569Z"
    }
   },
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented. \n",
    "# If you run this notebook on a different environment, e.g. your desktop, you may need to uncomment and install certain libraries.\n",
    "\n",
    "#!pip install numpy==1.21.4"
   ],
   "outputs": [],
   "execution_count": 143
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:18.740967Z",
     "start_time": "2025-02-23T06:17:18.738885Z"
    }
   },
   "source": [
    "import numpy as np # import Numpy library to generate \n",
    "\n",
    "weights = np.around(np.random.uniform(size=6), decimals=2) # initialize the weights\n",
    "biases = np.around(np.random.uniform(size=3), decimals=2) # initialize the biases"
   ],
   "outputs": [],
   "execution_count": 144
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the weights and biases for sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:20.071139Z",
     "start_time": "2025-02-23T06:17:20.068517Z"
    }
   },
   "source": [
    "print(weights)\n",
    "print(biases)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03 0.75 0.62 0.1  0.52 0.51]\n",
      "[0.29 0.25 0.18]\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the weights and the biases defined for the network, let's compute the output for a given input, $x_1$ and $x_2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:21.901178Z",
     "start_time": "2025-02-23T06:17:21.898608Z"
    }
   },
   "source": [
    "x_1 = 0.5 # input 1\n",
    "x_2 = 0.85 # input 2\n",
    "\n",
    "print('x1 is {} and x2 is {}'.format(x_1, x_2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 is 0.5 and x2 is 0.85\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by computing the wighted sum of the inputs, $z_{1, 1}$, at the first node of the hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:23.238976Z",
     "start_time": "2025-02-23T06:17:23.236711Z"
    }
   },
   "source": [
    "z_11 = x_1 * weights[0] + x_2 * weights[1] + biases[0]\n",
    "\n",
    "print('The weighted sum of the inputs at the first node in the hidden layer is {}'.format(z_11))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted sum of the inputs at the first node in the hidden layer is 0.9424999999999999\n"
     ]
    }
   ],
   "execution_count": 147
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:23.889487Z",
     "start_time": "2025-02-23T06:17:23.887170Z"
    }
   },
   "source": [
    "### type your answer here\n",
    "z_12 = x_1 * weights[2] + x_2 * weights[3] + biases[1]\n",
    "print(z_12)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.645\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's compute the weighted sum of the inputs, $z_{1, 2}$, at the second node of the hidden layer. Assign the value to **z_12**.\n",
    "z_12 = x_1 * weights[2] + x_2 * weights[3] + biases[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "z_12 = x_1 * weights[2] + x_2 * weights[3] + biases[1]\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the weighted sum.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:26.461Z",
     "start_time": "2025-02-23T06:17:26.458702Z"
    }
   },
   "source": [
    "print('The weighted sum of the inputs at the second node in the hidden layer is {}'.format(np.around(z_12, decimals=4)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted sum of the inputs at the second node in the hidden layer is 0.645\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, assuming a sigmoid activation function, let's compute the activation of the first node, $a_{1, 1}$, in the hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:27.801576Z",
     "start_time": "2025-02-23T06:17:27.799134Z"
    }
   },
   "source": [
    "a_11 = 1.0 / (1.0 + np.exp(-z_11))\n",
    "\n",
    "print('The activation of the first node in the hidden layer is {}'.format(np.around(a_11, decimals=4)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The activation of the first node in the hidden layer is 0.7196\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the activation of the second node, $a_{1, 2}$, in the hidden layer. Assign the value to **a_12**.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:29.409291Z",
     "start_time": "2025-02-23T06:17:29.407145Z"
    }
   },
   "source": [
    "### type your answer here\n",
    "a_12 = 1.0 / (1.0 + np.exp(-z_12))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 151
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "a_12 = 1.0 / (1.0 + np.exp(-z_12))\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the activation of the second node.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:31.537573Z",
     "start_time": "2025-02-23T06:17:31.535119Z"
    }
   },
   "source": [
    "print('The activation of the second node in the hidden layer is {}'.format(np.around(a_12, decimals=4)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The activation of the second node in the hidden layer is 0.6559\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now these activations will serve as the inputs to the output layer. So, let's compute the weighted sum of these inputs to the node in the output layer. Assign the value to **z_2**.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:32.961845Z",
     "start_time": "2025-02-23T06:17:32.959877Z"
    }
   },
   "source": [
    "### type your answer here\n",
    "z_2 = a_11 * weights[4] + a_12 * weights[5] + biases[2]"
   ],
   "outputs": [],
   "execution_count": 153
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "z_2 = a_11 * weights[4] + a_12 * weights[5] + biases[2]\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the weighted sum of the inputs at the node in the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:34.773511Z",
     "start_time": "2025-02-23T06:17:34.770730Z"
    }
   },
   "source": [
    "print('The weighted sum of the inputs at the node in the output layer is {}'.format(np.around(z_2, decimals=4)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted sum of the inputs at the node in the output layer is 0.8887\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compute the output of the network as the activation of the node in the output layer. Assign the value to **a_2**.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:36.111135Z",
     "start_time": "2025-02-23T06:17:36.109097Z"
    }
   },
   "source": [
    "### type your answer here\n",
    "a_2 = 1.0 / (1.0 + np.exp(-z_2))\n"
   ],
   "outputs": [],
   "execution_count": 155
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "a_2 = 1.0 / (1.0 + np.exp(-z_2))\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the activation of the node in the output layer which is equivalent to the prediction made by the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:38.517329Z",
     "start_time": "2025-02-23T06:17:38.515052Z"
    }
   },
   "source": [
    "print('The output of the network for x1 = 0.5 and x2 = 0.85 is {}'.format(np.around(a_2, decimals=4)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output of the network for x1 = 0.5 and x2 = 0.85 is 0.7086\n"
     ]
    }
   ],
   "execution_count": 156
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, neural networks for real problems are composed of many hidden layers and many more nodes in each layer. So, we can't continue making predictions using this very inefficient approach of computing the weighted sum at each node and the activation of each node manually. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to code an automatic way of making predictions, let's generalize our network. A general network would take $n$ inputs, would have many hidden layers, each hidden layer having $m$ nodes, and would have an output layer. Although the network is showing one hidden layer, but we will code the network to have many hidden layers. Similarly, although the network shows an output layer with one node, we will code the network to have more than one node in the output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cocl.us/general_neural_network\" alt=\"Neural Network General\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"item2\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item12'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by formally defining the structure of the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:17:49.921489Z",
     "start_time": "2025-02-23T06:17:49.919431Z"
    }
   },
   "source": [
    "n = 2 # number of inputs\n",
    "num_hidden_layers = 2 # number of hidden layers\n",
    "m = [2, 2] # number of nodes in each hidden layer\n",
    "num_nodes_output = 1 # number of nodes in the output layer"
   ],
   "outputs": [],
   "execution_count": 157
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Now that we defined the structure of the network, let's go ahead and initialize the weights and the biases in the network to random numbers. In order to be able to initialize the weights and the biases to random numbers, we will need to import the **Numpy** library.\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:18:06.381600Z",
     "start_time": "2025-02-23T06:18:06.377143Z"
    }
   },
   "source": [
    "import numpy as np # import the Numpy library\n",
    "\n",
    "num_nodes_previous = n # number of nodes in the previous layer\n",
    "\n",
    "network = {} # initialize network an an empty dictionary\n",
    "\n",
    "# Loop through each layer and randomly initialize the weights and biases associated with each node;\n",
    "# notice how we are adding 1 to the number of hidden layers in order to include the output layer\n",
    "for layer in range(num_hidden_layers + 1): \n",
    "    \n",
    "    # determine name of layer\n",
    "    if layer == num_hidden_layers:\n",
    "        layer_name = 'output'\n",
    "        num_nodes = num_nodes_output\n",
    "    else:\n",
    "        layer_name = 'layer_{}'.format(layer + 1)\n",
    "        num_nodes = m[layer]\n",
    "    \n",
    "    # initialize weights and biases associated with each node in the current layer\n",
    "    network[layer_name] = {}\n",
    "    for node in range(num_nodes):\n",
    "        node_name = 'node_{}'.format(node+1)\n",
    "        network[layer_name][node_name] = {\n",
    "            'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2),\n",
    "            'bias': np.around(np.random.uniform(size=1), decimals=2),\n",
    "        }\n",
    "    \n",
    "    num_nodes_previous = num_nodes\n",
    "    \n",
    "print(network) # print network"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_1': {'node_1': {'weights': array([0.58, 0.15]), 'bias': array([0.71])}, 'node_2': {'weights': array([0.86, 0.69]), 'bias': array([0.27])}}, 'layer_2': {'node_1': {'weights': array([0.4 , 0.51]), 'bias': array([0.34])}, 'node_2': {'weights': array([0.55, 0.69]), 'bias': array([0.3])}}, 'output': {'node_1': {'weights': array([0.22, 0.79]), 'bias': array([0.78])}}}\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:18:08.332178Z",
     "start_time": "2025-02-23T06:18:08.329472Z"
    }
   },
   "source": [
    "# The function below is a cleaner and more visually clear way to print the network structure\n",
    "def print_network_structure(network):\n",
    "    for layer, nodes in network.items():\n",
    "        print(f\"Layer: {layer}\")\n",
    "        for node, params in nodes.items():\n",
    "            print(f\"  {node}: Weights: {params['weights']} Bias: {params['bias']}\")\n",
    "        print()\n",
    "\n",
    "print_network_structure(network)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: layer_1\n",
      "  node_1: Weights: [0.58 0.15] Bias: [0.71]\n",
      "  node_2: Weights: [0.86 0.69] Bias: [0.27]\n",
      "\n",
      "Layer: layer_2\n",
      "  node_1: Weights: [0.4  0.51] Bias: [0.34]\n",
      "  node_2: Weights: [0.55 0.69] Bias: [0.3]\n",
      "\n",
      "Layer: output\n",
      "  node_1: Weights: [0.22 0.79] Bias: [0.78]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 159
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! So now with the above code, we are able to initialize the weights and the biases pertaining to any network of any number of hidden layers and number of nodes in each layer. But let's put this code in a function so that we are able to repetitively execute all this code whenever we want to construct a neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:18:25.347900Z",
     "start_time": "2025-02-23T06:18:25.344642Z"
    }
   },
   "source": [
    "def initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output):\n",
    "    \n",
    "    num_nodes_previous = num_inputs # number of nodes in the previous layer\n",
    "\n",
    "    network = {}\n",
    "    \n",
    "    # loop through each layer and randomly initialize the weights and biases associated with each layer\n",
    "    for layer in range(num_hidden_layers + 1):\n",
    "        \n",
    "        if layer == num_hidden_layers:\n",
    "            layer_name = 'output' # name last layer in the network output\n",
    "            num_nodes = num_nodes_output\n",
    "        else:\n",
    "            layer_name = 'layer_{}'.format(layer + 1) # otherwise give the layer a number\n",
    "            num_nodes = num_nodes_hidden[layer] \n",
    "        \n",
    "        # initialize weights and bias for each node\n",
    "        network[layer_name] = {}\n",
    "        for node in range(num_nodes):\n",
    "            node_name = 'node_{}'.format(node+1)\n",
    "            network[layer_name][node_name] = {\n",
    "                'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2),\n",
    "                'bias': np.around(np.random.uniform(size=1), decimals=2),\n",
    "            }\n",
    "    \n",
    "        num_nodes_previous = num_nodes\n",
    "\n",
    "    return network # return the network"
   ],
   "outputs": [],
   "execution_count": 160
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the *initialize_network* function to create a network that:\n",
    "\n",
    "1. takes 5 inputs\n",
    "2. has three hidden layers\n",
    "3. has 3 nodes in the first layer, 2 nodes in the second layer, and 3 nodes in the third layer\n",
    "4. has 1 node in the output layer\n",
    "\n",
    "Call the network **small_network**.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:18:41.101870Z",
     "start_time": "2025-02-23T06:18:41.098301Z"
    }
   },
   "source": [
    "### type your answer here\n",
    "n = 5 # number of inputs\n",
    "num_hidden_layers = 3 # number of hidden layers\n",
    "m = [3, 2, 3] # number of nodes in each hidden layer\n",
    "num_nodes_output = 1 # number of nodes in the output layer\n",
    "\n",
    "small_network = initialize_network(n, num_hidden_layers, m, num_nodes_output)\n",
    "\n",
    "print_network_structure(small_network)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: layer_1\n",
      "  node_1: Weights: [0.04 0.65 0.13 0.55 0.29] Bias: [0.83]\n",
      "  node_2: Weights: [0.1  0.9  0.34 0.13 0.46] Bias: [0.08]\n",
      "  node_3: Weights: [0.74 0.36 0.33 0.11 0.19] Bias: [0.52]\n",
      "\n",
      "Layer: layer_2\n",
      "  node_1: Weights: [0.11 0.48 0.93] Bias: [0.4]\n",
      "  node_2: Weights: [0.18 0.81 0.66] Bias: [0.61]\n",
      "\n",
      "Layer: layer_3\n",
      "  node_1: Weights: [0.12 0.35] Bias: [0.47]\n",
      "  node_2: Weights: [0.38 0.22] Bias: [0.23]\n",
      "  node_3: Weights: [0.14 0.73] Bias: [0.12]\n",
      "\n",
      "Layer: output\n",
      "  node_1: Weights: [0.32 0.39 0.04] Bias: [0.16]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 161
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"item3\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item13'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Weighted Sum at Each Node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted sum at each node is computed as the dot product of the inputs and the weights plus the bias. So let's create a function called *compute_weighted_sum* that does just that.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:18:45.314266Z",
     "start_time": "2025-02-23T06:18:45.311657Z"
    }
   },
   "source": [
    "def compute_weighted_sum(inputs, weights, bias):\n",
    "    return np.sum(inputs * weights) + bias"
   ],
   "outputs": [],
   "execution_count": 162
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate 5 inputs that we can feed to **small_network**.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:18:46.630820Z",
     "start_time": "2025-02-23T06:18:46.628317Z"
    }
   },
   "source": [
    "from random import seed\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12)\n",
    "inputs = np.around(np.random.uniform(size=5), decimals=2)\n",
    "\n",
    "print('The inputs to the network are {}'.format(inputs))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inputs to the network are [0.15 0.74 0.26 0.53 0.01]\n"
     ]
    }
   ],
   "execution_count": 163
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the *compute_weighted_sum* function to compute the weighted sum at the first node in the first hidden layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "node_weights = small_network['layer_1']['node_1']['weights']\n",
    "node_bias = small_network['layer_1']['node_1']['bias']\n",
    "\n",
    "weighted_sum = compute_weighted_sum(inputs, node_weights, node_bias)\n",
    "print('The weighted sum at the first node in the hidden layer is {}'.format(np.around(weighted_sum[0], decimals=4)))\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:18:49.083738Z",
     "start_time": "2025-02-23T06:18:49.081160Z"
    }
   },
   "source": [
    "### type your answer here\n",
    "node_weights = small_network['layer_1']['node_1']['weights']\n",
    "node_bias = small_network['layer_1']['node_1']['bias']\n",
    "\n",
    "weighted_sum = compute_weighted_sum(inputs, node_weights, node_bias)\n",
    "print('The weighted sum at the first node in the hidden layer is {}'.format(np.around(weighted_sum[0], decimals=4)))\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted sum at the first node in the hidden layer is 1.6452\n"
     ]
    }
   ],
   "execution_count": 164
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"item4\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item14'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Node Activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the output of each node is simply a non-linear tranformation of the weighted sum. We use activation functions for this mapping. Let's use the sigmoid function as the activation function here. So let's define a function that takes a weighted sum as input and returns the non-linear transformation of the input using the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:18:53.486053Z",
     "start_time": "2025-02-23T06:18:53.483661Z"
    }
   },
   "source": [
    "def node_activation(weighted_sum):\n",
    "    return 1.0 / (1.0 + np.exp(-1 * weighted_sum))"
   ],
   "outputs": [],
   "execution_count": 165
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the *node_activation* function to compute the output of the first node in the first hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:18:54.680904Z",
     "start_time": "2025-02-23T06:18:54.678469Z"
    }
   },
   "source": [
    "### type your answer here\n",
    "node1_output = node_activation(weighted_sum)\n",
    "print(node1_output)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83824126]\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "node_output  = node_activation(compute_weighted_sum(inputs, node_weights, node_bias))\n",
    "print('The output of the first node in the hidden layer is {}'.format(np.around(node_output[0], decimals=4)))\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"item5\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item15'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final piece of building a neural network that can perform predictions is to put everything together. So let's create a function that applies the *compute_weighted_sum* and *node_activation* functions to each node in the network and propagates the data all the way to the output layer and outputs a prediction for each node in the output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we are going to accomplish this is through the following procedure:\n",
    "\n",
    "1. Start with the input layer as the input to the first hidden layer.\n",
    "2. Compute the weighted sum at the nodes of the current layer.\n",
    "3. Compute the output of the nodes of the current layer.\n",
    "4. Set the output of the current layer to be the input to the next layer.\n",
    "5. Move to the next layer in the network.\n",
    "5. Repeat steps 2 - 4 until we compute the output of the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:19:00.392448Z",
     "start_time": "2025-02-23T06:19:00.389204Z"
    }
   },
   "source": [
    "def forward_propagate(network, inputs):\n",
    "    \n",
    "    layer_inputs = list(inputs) # start with the input layer as the input to the first hidden layer\n",
    "    \n",
    "    for layer in network:\n",
    "        \n",
    "        layer_data = network[layer]\n",
    "        \n",
    "        layer_outputs = [] \n",
    "        \n",
    "        for layer_node in layer_data:\n",
    "        \n",
    "            node_data = layer_data[layer_node]\n",
    "        \n",
    "            # compute the weighted sum and the output of each node at the same time \n",
    "            node_output = node_activation(compute_weighted_sum(layer_inputs, node_data['weights'], node_data['bias']))\n",
    "            layer_outputs.append(np.around(node_output[0], decimals=4))\n",
    "            \n",
    "        if layer != 'output':\n",
    "            print('The outputs of the nodes in hidden layer number {} is {}'.format(layer.split('_')[1], [float(val) for val in layer_outputs]))\n",
    "        \n",
    "        layer_inputs = layer_outputs # set the output of this layer to be the input to next layer\n",
    "\n",
    "    network_predictions = layer_outputs\n",
    "    return network_predictions"
   ],
   "outputs": [],
   "execution_count": 167
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the *forward_propagate* function to compute the prediction of our small network\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-02-23T06:19:03.139660Z",
     "start_time": "2025-02-23T06:19:03.137042Z"
    }
   },
   "source": [
    "### type your answser here\n",
    "\n",
    "predictions = forward_propagate(small_network, inputs)\n",
    "print('\\nThe predicted value by the network for the given input is {}'.format(np.around(predictions[0], decimals=4)))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The outputs of the nodes in hidden layer number 1 is [0.8382, 0.7156, 0.7395]\n",
      "The outputs of the nodes in hidden layer number 2 is [0.821, 0.8616]\n",
      "The outputs of the nodes in hidden layer number 3 is [0.7048, 0.6751, 0.7035]\n",
      "\n",
      "The predicted value by the network for the given input is 0.6631\n"
     ]
    }
   ],
   "execution_count": 168
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "predictions = forward_propagate(small_network, inputs)\n",
    "print('The predicted value by the network for the given input is {}'.format(np.around(predictions[0], decimals=4)))\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we built the code to define a neural network. We can specify the number of inputs that a neural network can take, the number of hidden layers as well as the number of nodes in each hidden layer, and the number of nodes in the output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use the *initialize_network* to create our neural network and define its weights and biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:19:06.630456Z",
     "start_time": "2025-02-23T06:19:06.628104Z"
    }
   },
   "source": [
    "my_network = initialize_network(5, 3, [2, 3, 2], 3)"
   ],
   "outputs": [],
   "execution_count": 169
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for a given input,\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:19:07.900587Z",
     "start_time": "2025-02-23T06:19:07.898499Z"
    }
   },
   "source": [
    "inputs = np.around(np.random.uniform(size=5), decimals=2)"
   ],
   "outputs": [],
   "execution_count": 170
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we compute the network predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:19:09.075823Z",
     "start_time": "2025-02-23T06:19:09.073143Z"
    }
   },
   "source": [
    "predictions = forward_propagate(my_network, inputs)\n",
    "print('The predicted values by the network for the given input are {}'.format(predictions))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The outputs of the nodes in hidden layer number 1 is [0.8857, 0.8889]\n",
      "The outputs of the nodes in hidden layer number 2 is [0.7822, 0.6965, 0.7411]\n",
      "The outputs of the nodes in hidden layer number 3 is [0.868, 0.881]\n",
      "The predicted values by the network for the given input are [np.float64(0.8952), np.float64(0.8222), np.float64(0.8035)]\n"
     ]
    }
   ],
   "execution_count": 171
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with the code by creating different networks of different structures and enjoy making predictions using the *forward_propagate* function.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:19:14.225321Z",
     "start_time": "2025-02-23T06:19:14.219688Z"
    }
   },
   "source": [
    "# Example activation function (Sigmoid)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def forward_propagate(network, inputs, activation_function):\n",
    "    \"\"\"\n",
    "    Performs forward propagation through the neural network.\n",
    "\n",
    "    Parameters:\n",
    "        network (dict): The neural network structure with weights and biases.\n",
    "        inputs (array-like): Input values to the network.\n",
    "        activation_function (function): Function to apply at each node.\n",
    "\n",
    "    Returns:\n",
    "        list: Final output of the network.\n",
    "    \"\"\"\n",
    "    \n",
    "    layer_inputs = np.array(inputs)  # Convert to NumPy array for efficiency\n",
    "\n",
    "    for layer_name, neurons in network.items():  # Iterate through layers\n",
    "        \n",
    "        layer_outputs = []  # Store outputs of this layer\n",
    "        \n",
    "        for neuron_name, neuron_params in neurons.items():  # Iterate through neurons\n",
    "            weights = np.array(neuron_params['weights'])\n",
    "            bias = neuron_params['bias']\n",
    "            \n",
    "            # Compute weighted sum and apply activation function\n",
    "            weighted_sum = np.dot(layer_inputs, weights) + bias\n",
    "            neuron_output = activation_function(weighted_sum)\n",
    "            \n",
    "            layer_outputs.append(np.around(neuron_output[0], decimals=4))\n",
    "        \n",
    "        if \"output\" not in layer_name:\n",
    "            print(f\"Outputs of {layer_name}: {[float(val) for val in layer_outputs]}\")\n",
    "            \n",
    "        \n",
    "        layer_inputs = np.array(layer_outputs)  # Pass outputs as inputs to next layer\n",
    "\n",
    "    return layer_outputs  # Final predictions\n",
    "\n",
    "\n",
    "num_inputs = 5 # number of inputs\n",
    "num_hidden_layers = 3 # number of hidden layers\n",
    "num_nodes_hidden = [3, 2, 3] # number of nodes in each hidden layer\n",
    "num_nodes_output = 1 # number of nodes in the output layer\n",
    "\n",
    "inputs = np.around(np.random.uniform(size=5), decimals=2)\n",
    "\n",
    "network = initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output)\n",
    "\n",
    "print_network_structure(network)\n",
    "\n",
    "# Forward Propagation\n",
    "predictions = forward_propagate(network, inputs, sigmoid)\n",
    "\n",
    "print(\"\\nFinal Network Prediction:\", float(predictions[0]))\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: layer_1\n",
      "  node_1: Weights: [0.03 0.3  0.22 0.06 0.52] Bias: [0.42]\n",
      "  node_2: Weights: [0.05 0.57 0.8  0.11 0.28] Bias: [0.64]\n",
      "  node_3: Weights: [0.49 0.51 0.46 0.89 0.61] Bias: [0.6]\n",
      "\n",
      "Layer: layer_2\n",
      "  node_1: Weights: [0.44 0.48 0.89] Bias: [0.21]\n",
      "  node_2: Weights: [0.94 0.07 0.6 ] Bias: [0.03]\n",
      "\n",
      "Layer: layer_3\n",
      "  node_1: Weights: [0.67 0.64] Bias: [0.86]\n",
      "  node_2: Weights: [0.94 0.45] Bias: [0.67]\n",
      "  node_3: Weights: [0.92 0.62] Bias: [0.32]\n",
      "\n",
      "Layer: output\n",
      "  node_1: Weights: [0.32 0.29 0.96] Bias: [0.41]\n",
      "\n",
      "Outputs of layer_1: [0.6985, 0.733, 0.873]\n",
      "Outputs of layer_2: [0.8384, 0.7793]\n",
      "Outputs of layer_3: [0.8722, 0.8592, 0.8284]\n",
      "\n",
      "Final Network Prediction: 0.8499\n"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:19:18.409145Z",
     "start_time": "2025-02-23T06:19:18.351942Z"
    }
   },
   "source": [
    "from sympy import *\n",
    "'''\n",
    "The Rectified Linear Unit (ReLU) is a widely used activation function in artificial neural networks, particularly in deep learning models. \n",
    "Its purpose is to introduce non-linearity into the model while maintaining computational efficiency. Here are some key reasons why ReLU \n",
    "is popular and useful:\n",
    "\n",
    "Non-linearity: Although ReLU looks like a linear function, it introduces non-linearity into the model. This is crucial because it allows \n",
    "the neural network to learn complex patterns and representations. Without non-linear activation functions, a neural network would behave \n",
    "like a linear model, regardless of its depth.\n",
    "\n",
    "Simplicity and Efficiency: ReLU is computationally efficient because it involves simple thresholding at zero. This simplicity allows for \n",
    "faster training and inference compared to other activation functions like sigmoid or hyperbolic tangent (tanh), which involve more complex \n",
    "mathematical operations.\n",
    "\n",
    "Sparsity: ReLU activation can lead to sparse representations. Since it outputs zero for any negative input, many neurons may not activate \n",
    "(i.e., output zero) for a given input. This sparsity can lead to more efficient models by reducing the number of active neurons and can \n",
    "help mitigate the vanishing gradient problem.\n",
    "\n",
    "Mitigation of the Vanishing Gradient Problem: The vanishing gradient problem is a challenge in training deep neural networks, where gradients\n",
    "become very small, effectively preventing the network from learning. ReLU helps mitigate this issue because its gradient is either zero \n",
    "(for negative inputs) or one (for positive inputs), which helps maintain a stronger gradient signal during backpropagation.\n",
    "\n",
    "Empirical Success: ReLU has been empirically shown to work well in practice, especially in deep networks. It has become the default \n",
    "activation function for many architectures, such as convolutional neural networks (CNNs).'''\n",
    "\n",
    "# plot Rectified Linear Unit (ReLu)\n",
    "x = symbols('x')\n",
    "relu = Max(0, x)\n",
    "plot(relu)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<sympy.plotting.backends.matplotlibbackend.matplotlib.MatplotlibBackend at 0x320f93950>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 173
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:19:19.335800Z",
     "start_time": "2025-02-23T06:19:19.328415Z"
    }
   },
   "source": [
    "# I copied this example from pages 237-238 from Essential Math for Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_data = pd.read_csv(\"light_dark_font_training_set.csv\")\n",
    "#all_data = all_data[:50]\n",
    "#print(all_data)\n",
    "\n",
    "# Extract the input columns; scale down by 255\n",
    "X = (all_data.iloc[:, 0:3].values/255.0)\n",
    "y = (all_data.iloc[:, -1].values)\n",
    "\n",
    "# Split training and testing data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.2, random_state=4)\n",
    "\n",
    "n = X_train.shape[0] # Number of training records\n",
    "\n",
    "# Build neural network with weights and biases\n",
    "w_hidden = np.random.rand(3, 3)\n",
    "w_output = np.random.rand(1, 3)\n",
    "b_hidden = np.random.rand(3, 1)\n",
    "b_output = np.random.rand(1, 1)\n",
    "\n",
    "# Activation functions\n",
    "relu = lambda x: np.maximum(x, 0)\n",
    "logistic = lambda x: 1/(1 + np.exp(-x))\n",
    "\n",
    "# Run forward propagation through the neural network to get predicted outputs.\n",
    "# Note: the '@' operator the matrix multiplication operator in Python\n",
    "def forward_prop(X):\n",
    "    #print(\"*****************\")\n",
    "    #print(X)\n",
    "    Z1 = w_hidden @ X + b_hidden\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = w_output @ A1 + b_output\n",
    "    A2 = logistic(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# Calculate accuracy\n",
    "#print(X_test)\n",
    "test_predictions = forward_prop(X_test.transpose())[3] # Retrieve output layer (i.e. A2)\n",
    "#print(test_predictions)\n",
    "test_comparisons = np.equal((test_predictions >= .5).flatten().astype(int), y_test)\n",
    "accuracy = sum(test_comparisons.astype(int) / X_test.shape[0])\n",
    "print(\"\\nAccuracy: {0:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "#print(test_predictions)\n",
    "#print(test_comparisons)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 60.59%\n"
     ]
    }
   ],
   "execution_count": 174
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:26:23.618057Z",
     "start_time": "2025-02-23T06:26:23.611540Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the neural network with random weights and biases\n",
    "def initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output):\n",
    "    num_nodes_previous = num_inputs\n",
    "    network = {}\n",
    "\n",
    "    for layer in range(num_hidden_layers + 1):\n",
    "        if layer == num_hidden_layers:\n",
    "            layer_name = 'output'\n",
    "            num_nodes = num_nodes_output\n",
    "        else:\n",
    "            layer_name = f'layer_{layer + 1}'\n",
    "            num_nodes = num_nodes_hidden[layer]\n",
    "\n",
    "        network[layer_name] = {\n",
    "            'weights': np.random.randn(num_nodes, num_nodes_previous),  # Initialize weights\n",
    "            'biases': np.zeros((num_nodes, 1))  # Initialize biases as zero\n",
    "        }\n",
    "\n",
    "        num_nodes_previous = num_nodes\n",
    "\n",
    "    return network\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Forward propagation through the network\n",
    "def forward_propagate(network, inputs, activation_function):\n",
    "    layer_inputs = inputs.T  # Transpose to match weight dimensions (n_features, n_samples)\n",
    "\n",
    "    for layer_name, layer_params in network.items():\n",
    "        weights = layer_params['weights']\n",
    "        biases = layer_params['biases']\n",
    "\n",
    "        # Compute the weighted sum\n",
    "        weighted_sum = np.dot(weights, layer_inputs) + biases  # Matrix multiplication\n",
    "        layer_outputs = activation_function(weighted_sum)  # Apply activation function\n",
    "\n",
    "        # Store the outputs of this layer for the next layer\n",
    "        layer_inputs = layer_outputs\n",
    "\n",
    "    return layer_inputs.T  # Return the final output (transpose back)\n",
    "\n",
    "\n",
    "# Sample data: 18 samples with RGB features and target labels\n",
    "data = np.array([\n",
    "    [0, 0, 0, 0],\n",
    "    [0, 0, 128, 0],\n",
    "    [0, 0, 139, 0],\n",
    "    [0, 0, 205, 0],\n",
    "    [0, 0, 238, 0],\n",
    "    [0, 0, 255, 0],\n",
    "    [0, 100, 0, 0],\n",
    "    [0, 104, 139, 0],\n",
    "    [0, 128, 0, 0],\n",
    "    [0, 128, 128, 0],\n",
    "    [0, 134, 139, 0],\n",
    "    [0, 139, 0, 0],\n",
    "    [0, 139, 139, 0],\n",
    "    [0, 139, 69, 0],\n",
    "    [0, 154, 205, 0],\n",
    "    [0, 178, 238, 1],\n",
    "    [0, 191, 255, 1],\n",
    "    [0, 197, 205, 1]\n",
    "])\n",
    "\n",
    "# Split data into inputs (X) and labels (y)\n",
    "X = data[:, :-1]  # All rows, first 3 columns (RGB values)\n",
    "y = data[:, -1]   # All rows, last column (light/dark label)\n",
    "\n",
    "# Network configuration\n",
    "num_inputs = X.shape[1]  # 3 (RGB)\n",
    "num_hidden_layers = 3\n",
    "num_nodes_hidden = [4, 3, 3]  # Example hidden layer sizes\n",
    "num_nodes_output = 1  # Single output node (light/dark)\n",
    "\n",
    "# Initialize the network\n",
    "network = initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output)\n",
    "\n",
    "# Forward propagation for a single input\n",
    "predictions = forward_propagate(network, X, sigmoid)\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(predictions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "[[0.61126731]\n",
      " [0.60988055]\n",
      " [0.60988055]\n",
      " [0.60988055]\n",
      " [0.60988055]\n",
      " [0.60988055]\n",
      " [0.61063103]\n",
      " [0.61063102]\n",
      " [0.61063103]\n",
      " [0.61063103]\n",
      " [0.61063103]\n",
      " [0.61063103]\n",
      " [0.61063103]\n",
      " [0.61063103]\n",
      " [0.61063103]\n",
      " [0.61063103]\n",
      " [0.61063103]\n",
      " [0.61063103]]\n"
     ]
    }
   ],
   "execution_count": 182
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T06:25:50.134317Z",
     "start_time": "2025-02-23T06:25:50.106657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sigmoid and its derivative for activation and backpropagation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Initialize network\n",
    "def initialize_network(input_size, hidden_size, output_size):\n",
    "    network = {\n",
    "        'layer_1': {\n",
    "            'weights': np.random.randn(hidden_size, input_size),\n",
    "            'biases': np.zeros((hidden_size, 1))\n",
    "        },\n",
    "        'layer_2': {\n",
    "            'weights': np.random.randn(hidden_size, hidden_size),\n",
    "            'biases': np.zeros((hidden_size, 1))\n",
    "        },\n",
    "        'output': {\n",
    "            'weights': np.random.randn(output_size, hidden_size),\n",
    "            'biases': np.zeros((output_size, 1))\n",
    "        }\n",
    "    }\n",
    "    return network\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagate(network, X, activation_func):\n",
    "    layer_1_input = X.dot(network['layer_1']['weights'].T) + network['layer_1']['biases'].T\n",
    "    layer_1_output = activation_func(layer_1_input)\n",
    "\n",
    "    layer_2_input = layer_1_output.dot(network['layer_2']['weights'].T) + network['layer_2']['biases'].T\n",
    "    layer_2_output = activation_func(layer_2_input)\n",
    "\n",
    "    output_input = layer_2_output.dot(network['output']['weights'].T) + network['output']['biases'].T\n",
    "    output = activation_func(output_input)\n",
    "\n",
    "    return output, [layer_1_output, layer_2_output]\n",
    "\n",
    "# Backpropagation to update weights and biases\n",
    "def backpropagate(network, X, y, learning_rate):\n",
    "    # Forward propagation to get outputs\n",
    "    output, [layer_1_output, layer_2_output] = forward_propagate(network, X, sigmoid)\n",
    "\n",
    "    # Calculate the error at the output layer\n",
    "    error = y - output\n",
    "    dz = error * sigmoid_derivative(output)  # Shape: (1345, 1)\n",
    "\n",
    "    # Update weights and biases for the output layer\n",
    "    network['output']['weights'] += learning_rate * layer_2_output.T.dot(dz)  # Shape: (4, 1345).dot(1345, 1) => (4, 1)\n",
    "    network['output']['biases'] += learning_rate * np.sum(dz, axis=0, keepdims=True)  # Bias update for the output layer, Shape: (1, 1)\n",
    "\n",
    "    # Backpropagate the error to layer 2\n",
    "    dz_next_layer = dz.dot(network['output']['weights'].T)  # (1345, 1) dot (1, 4) => (1345, 4)\n",
    "    dz = dz_next_layer * sigmoid_derivative(layer_2_output)  # Shape: (1345, 4)\n",
    "    network['layer_2']['weights'] += learning_rate * layer_1_output.T.dot(dz)  # Shape: (3, 4)\n",
    "    network['layer_2']['biases'] += learning_rate * np.sum(dz, axis=0, keepdims=True)  # Bias update for layer 2\n",
    "\n",
    "    # Backpropagate to layer 1\n",
    "    dz_next_layer = dz.dot(network['layer_2']['weights'].T)  # (1345, 4) dot (4, 3) => (1345, 3)\n",
    "    dz = dz_next_layer * sigmoid_derivative(layer_1_output)  # Shape: (1345, 3)\n",
    "    network['layer_1']['weights'] += learning_rate * X.T.dot(dz)  # Shape: (3, 3)\n",
    "    network['layer_1']['biases'] += learning_rate * np.sum(dz, axis=0, keepdims=True)  # Bias update for layer 1\n",
    "\n",
    "    return error\n",
    "\n",
    "# Train the network\n",
    "def train_network(network, X, y, epochs=1000, learning_rate=0.01):\n",
    "    for epoch in range(epochs):\n",
    "        error = backpropagate(network, X, y, learning_rate)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Mean Squared Error = {np.mean(error ** 2)}\")\n",
    "\n",
    "# Load the data from the CSV file\n",
    "def load_data(file_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "    # Extract features (RED, GREEN, BLUE) and target (LIGHT_OR_DARK_FONT_IND)\n",
    "    X = df[['RED', 'GREEN', 'BLUE']].values\n",
    "    y = df['LIGHT_OR_DARK_FONT_IND'].values.reshape(-1, 1)\n",
    "    return X, y\n",
    "\n",
    "# Load the dataset\n",
    "file_name = 'light_dark_font_training_set.csv'  # Replace with your actual file path\n",
    "X, y = load_data(file_name)\n",
    "\n",
    "# Initialize the network with input size = 3 (RGB), hidden size = 4, and output size = 1 (binary classification)\n",
    "network = initialize_network(input_size=3, hidden_size=4, output_size=1)\n",
    "\n",
    "# Train the network with the dataset\n",
    "train_network(network, X, y, epochs=1000, learning_rate=0.01)\n",
    "\n",
    "# Make predictions with the trained network\n",
    "predictions, _ = forward_propagate(network, X, sigmoid)\n",
    "print(\"Predictions: \", predictions)\n"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,4) doesn't match the broadcast shape (4,4)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[179], line 92\u001B[0m\n\u001B[1;32m     89\u001B[0m network \u001B[38;5;241m=\u001B[39m initialize_network(input_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, hidden_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, output_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     91\u001B[0m \u001B[38;5;66;03m# Train the network with the dataset\u001B[39;00m\n\u001B[0;32m---> 92\u001B[0m \u001B[43mtrain_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;66;03m# Make predictions with the trained network\u001B[39;00m\n\u001B[1;32m     95\u001B[0m predictions, _ \u001B[38;5;241m=\u001B[39m forward_propagate(network, X, sigmoid)\n",
      "Cell \u001B[0;32mIn[179], line 72\u001B[0m, in \u001B[0;36mtrain_network\u001B[0;34m(network, X, y, epochs, learning_rate)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mtrain_network\u001B[39m(network, X, y, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m):\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m---> 72\u001B[0m         error \u001B[38;5;241m=\u001B[39m \u001B[43mbackpropagate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     74\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: Mean Squared Error = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnp\u001B[38;5;241m.\u001B[39mmean(error\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[179], line 52\u001B[0m, in \u001B[0;36mbackpropagate\u001B[0;34m(network, X, y, learning_rate)\u001B[0m\n\u001B[1;32m     49\u001B[0m dz \u001B[38;5;241m=\u001B[39m error \u001B[38;5;241m*\u001B[39m sigmoid_derivative(output)  \u001B[38;5;66;03m# Shape: (1345, 1)\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# Update weights and biases for the output layer\u001B[39;00m\n\u001B[0;32m---> 52\u001B[0m \u001B[43mnetwork\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moutput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweights\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlayer_2_output\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdz\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Shape: (4, 1345).dot(1345, 1) => (4, 1)\u001B[39;00m\n\u001B[1;32m     53\u001B[0m network[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbiases\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m learning_rate \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39msum(dz, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# Bias update for the output layer, Shape: (1, 1)\u001B[39;00m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# Backpropagate the error to layer 2\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: non-broadcastable output operand with shape (1,4) doesn't match the broadcast shape (4,4)"
     ]
    }
   ],
   "execution_count": 179
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-21  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\">  IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *Introduction to Deep Learning & Neural Networks with Keras*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0101EN_Coursera_Week1_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2019 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
