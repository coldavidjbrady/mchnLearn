{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **In-Context Engineering and Prompt Templates**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're stepping into the world of prompt engineering, where each command you craft has the power to guide intelligent LLM systems toward specific outcomes. In this tutorial, you will explore the foundational aspects of prompt engineering, dive into advanced techniques of in-context learning, such as few-shot and self-consistent learning, and learn how to effectively use tools like Langchain.\n",
    "\n",
    "Start by understanding the basics—how to formulate prompts that communicate effectively with AI. From there, we'll explore how the Langchain prompt template can simplify and enhance this process, making it more structured and efficient.\n",
    "\n",
    "As you progress, you'll learn to apply these skills in practical scenarios, creating sophisticated applications like QA bots and text summarization tools. By using the Langchain prompt template, you'll see firsthand how structured prompting can streamline the development of these applications, transforming complex requirements into clear, concise tasks for AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ai8G4tOU4mksEYfv5wsghA/prompt%20engineering.png\" width=\"50%\" alt=\"indexing\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this tutorial, you'll not only master the different techniques of prompt engineering but also acquire hands-on experience in applying these techniques to real-world problems, ensuring you're well-prepared to harness the full potential of AI in various settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "            <li><a href=\"#Setup-LLM\">Setup LLM</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Prompt-engineering\">Prompt engineering</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#First-basic-prompt\">First basic prompt</a></li>\n",
    "            <li><a href=\"#Zero-shot-prompt\">Zero-shot prompt</a></li>\n",
    "            <li><a href=\"#One-shot-prompt\">One-shot prompt</a></li>\n",
    "            <li><a href=\"#Few-shot-prompt\">Few-shot prompt</a></li>\n",
    "            <li><a href=\"#Chain-of-thought-(CoT)-prompting\">Chain-of-thought (CoT) prompting</a></li>\n",
    "            <li><a href=\"#Self-consistency\">Self-consistency</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Applications\">Applications</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Prompt-template\">Prompt template</a></li>\n",
    "            <li><a href=\"#Text-summarization\">Text summarization</a></li>\n",
    "            <li><a href=\"#Question-answering\">Question answering</a></li>\n",
    "            <li><a href=\"#Text-classification\">Text classification</a></li>\n",
    "            <li><a href=\"#Code-generation\">Code generation</a></li>\n",
    "            <li><a href=\"#Role-playing\">Role playing</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<a href=\"#Exercises\">Exercises</a>\n",
    "<ol>\n",
    "    <li><a href=\"#Exercise-1:-Change-parameters-for-the-LLM\">Exercise 1: Change parameters for the LLM</a></li>\n",
    "    <li><a href=\"#Exercise-2:-Observe-how-LLM-thinks\">Exercise 2: Observe how LLM thinks</a></li>\n",
    "    <li><a href=\"#Exercise-3:-Revise-the-text-classification-agent-to-one-shot-learning\">Exercise 3: Revise the text classification agent to one-shot learning</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- **Understand the basics of prompt engineering**: Gain a solid foundation in how to effectively communicate with LLM using prompts, setting the stage for more advanced techniques.\n",
    "\n",
    "- **Master advanced prompt techniques**: Learn and apply advanced prompt engineering techniques such as few-shot and self-consistent learning to optimize the LLM's response.\n",
    "\n",
    "- **Utilize LangChain prompt template**: Become proficient in using LangChain's prompt template to structure and optimize your interactions with LLM.\n",
    "\n",
    "- **Develop practical LLM agents**: Acquire the skills to create and implement agents such as QA bots and text summarization using the Langchain prompt template, translating theoretical knowledge into practical solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "*   [`ibm-watsonx-ai`](https://ibm.github.io/watson-machine-learning-sdk/index.html) for using LLMs from IBM's watsonx.ai.\n",
    "*   [`langchain`](https://www.langchain.com/) for using langchain's different chain and prompt functions.\n",
    "*   [`langchain-ibm`](https://python.langchain.com/v0.1/docs/integrations/llms/ibm_watsonx/) provides integration between langchain and ibm-watsonx-ai.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T19:49:25.744019Z",
     "start_time": "2025-12-22T19:49:25.733719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import IPython\n",
    "\n",
    "#IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "load_dotenv(override=True)  # override=True matters here\n",
    "\n",
    "print(\"OPENAI_API_KEY loaded?\", bool(os.getenv(\"OPENAI_API_KEY\")))\n",
    "print(\"Key prefix:\", os.getenv(\"OPENAI_API_KEY\")[:6])\n",
    "print(\"Key length:\", len(os.getenv(\"OPENAI_API_KEY\")))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded? True\n",
      "Key prefix: sk-pro\n",
      "Key length: 164\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ONE-CELL BOOTSTRAP (safe + conditional):\n",
    "# - Loads OPENAI_API_KEY from a .env file (no hardcoding)\n",
    "# - Installs required packages ONLY if missing/broken\n",
    "# - Restarts kernel ONLY if installs occurred\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib.util\n",
    "import IPython\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Load environment (.env)\n",
    "# ----------------------------\n",
    "def _ensure_env_loaded():\n",
    "    # If python-dotenv isn't installed yet, we'll install it later (only if needed).\n",
    "    if importlib.util.find_spec(\"dotenv\") is not None:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()  # loads .env from current working dir by default\n",
    "\n",
    "_ensure_env_loaded()\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Check required modules\n",
    "# ----------------------------\n",
    "print(\"Kernel Python:\", sys.executable)\n",
    "\n",
    "REQUIRED = {\n",
    "    \"openai\": \"openai\",\n",
    "    \"langchain\": \"langchain\",\n",
    "    \"langchain_core\": \"langchain-core\",\n",
    "    \"langchain_community\": \"langchain-community\",\n",
    "    \"langchain_openai\": \"langchain-openai\",\n",
    "    # Optional but recommended for .env support\n",
    "    \"dotenv\": \"python-dotenv\",\n",
    "}\n",
    "\n",
    "def _has_module(modname: str) -> bool:\n",
    "    return importlib.util.find_spec(modname) is not None\n",
    "\n",
    "missing_pkgs = [pkg for mod, pkg in REQUIRED.items() if not _has_module(mod)]\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Install only if missing\n",
    "# ----------------------------\n",
    "did_install = False\n",
    "if missing_pkgs:\n",
    "    print(\"Missing packages detected, installing:\", \", \".join(missing_pkgs))\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", *missing_pkgs])\n",
    "    did_install = True\n",
    "else:\n",
    "    print(\"All required packages already installed ✅\")\n",
    "\n",
    "# If we just installed python-dotenv, load .env now\n",
    "if did_install and _has_module(\"dotenv\"):\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Restart kernel ONLY if installs happened\n",
    "# ----------------------------\n",
    "if did_install:\n",
    "    print(\"Environment updated. Restarting kernel…\")\n",
    "IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Validate API key (after env load)\n",
    "# ----------------------------\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY is not set.\\n\"\n",
    "        \"Create a .env file next to your notebook (or project root) containing:\\n\"\n",
    "        \"  OPENAI_API_KEY=sk-...\\n\"\n",
    "        \"Also ensure '.env' is in .gitignore.\"\n",
    "    )\n",
    "\n",
    "print(\"OPENAI_API_KEY loaded ✅ (prefix:\", api_key[:6], \"…)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prompt engineering\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### First basic prompt\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this example, let's introduce a basic prompt that utilizes specific parameters to guide the language model's response. You'll then define a simple prompt and retrieve the model's response,\n",
    "\n",
    "The prompt used is \"The wind is\". Let the model generate itself.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:00:53.980537Z",
     "start_time": "2025-12-22T20:00:51.212863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ----------------------------\n",
    "# 5) Smoke tests: OpenAI SDK + LangChain\n",
    "# ----------------------------\n",
    "\n",
    "# A) OpenAI SDK (fast sanity check)\n",
    "client = OpenAI()\n",
    "resp = client.responses.create(model=\"gpt-4o-mini\", input=\"Reply with exactly: OK\")\n",
    "print(\"[OpenAI SDK] ->\", resp.output_text.strip())\n",
    "\n",
    "# B) LangChain ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2, max_tokens=64)\n",
    "print(\"[LangChain] ->\", llm.invoke(\"Reply with exactly: OK\").content.strip())\n",
    "\n",
    "# C) Modern LCEL chain: prompt | llm (LCEL = LangChain Expression Language)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are concise.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "# Pipe Operator below (|) - Take the output of the left thing and feed it into the right thing...this is a chain\n",
    "chain = prompt | llm\n",
    "out = chain.invoke({\"question\": \"Explain RAG in one sentence.\"})\n",
    "print(\"[LCEL chain] ->\", out.content.strip())\n",
    "\n",
    "print(\"\\n✅ Environment + .env + OpenAI + LangChain are all working.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OpenAI SDK] -> OK\n",
      "[LangChain] -> OK\n",
      "[LCEL chain] -> RAG, or Retrieval-Augmented Generation, is a machine learning approach that combines information retrieval with natural language generation to enhance the quality and relevance of generated text by incorporating external knowledge sources.\n",
      "\n",
      "✅ Environment + .env + OpenAI + LangChain are all working.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''As you can see from the response, the model continues generating content following the initial prompt, \"The wind is\". You might notice that the response appears truncated or incomplete. This is because you have set the `max_new_tokens,` which restricts the number of tokens the model can generate.\n",
    "\n",
    "Try to adjust the parameters and observe the difference in the response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a zero-shot prompt. \n",
    "\n",
    "Zero-shot learning is crucial for testing a model's ability to apply its pre-trained knowledge to new, unseen tasks without additional training. This capability is valuable for gauging the model's generalization skills.\n",
    "\n",
    "In this example, let's demonstrate a zero-shot learning scenario using a prompt that asks the model to classify a statement without any prior specific training on similar tasks. The prompt requests the model to assess the truthfulness of the statement: \"The Eiffel Tower is located in Berlin.\". After defining the prompt, you'll execute it with default parameters and print the response.\n",
    "\n",
    "This approach helps you understand how well the model can handle direct questions based on its underlying knowledge and reasoning abilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the prompt to see the model's capacity to correctly analyze and respond to factual inaccuracies.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:23:39.377111Z",
     "start_time": "2025-12-22T20:23:38.706755Z"
    }
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_tmpl = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"\"\"Classify the following statement as true or false:\n",
    "    '{statement}'\n",
    "\n",
    "    Answer:\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "chain = prompt_tmpl | llm\n",
    "\n",
    "out = chain.invoke({\n",
    "    \"statement\": \"The Eiffel Tower is located in Berlin.\"\n",
    "})\n",
    "\n",
    "print(\"prompt:\\n\", prompt_tmpl.format(statement=\"The Eiffel Tower is located in Berlin.\"))\n",
    "print(\"\\nresponse:\\n\", out.content)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      " Human: Classify the following statement as true or false:\n",
      "    'The Eiffel Tower is located in Berlin.'\n",
      "\n",
      "    Answer:\n",
      "    \n",
      "\n",
      "response:\n",
      " False. The Eiffel Tower is located in Paris, France, not Berlin.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model responds with the 'False' answer, which is correct. It also gives the reason for it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-shot prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a one-shot learning example where the model is given a single example to help guide its translation from English to French.\n",
    "\n",
    "The prompt provides a sample translation pairing, \"How is the weather today?\" translated to \"Comment est le temps aujourd'hui?\" This example serves as a guide for the model to understand the task context and desired format. The model is then tasked with translating a new sentence, \"Where is the nearest supermarket?\" without further guidance.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:25:36.188178Z",
     "start_time": "2025-12-22T20:25:35.528181Z"
    }
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt template with a variable\n",
    "prompt_tmpl = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"\"\"Here is an example of translating a sentence from English to French:\n",
    "\n",
    "English: \"How is the weather today?\"\n",
    "French: \"Comment est le temps aujourd'hui?\"\n",
    "\n",
    "Now, translate the following sentence from English to French:\n",
    "\n",
    "English: \"{sentence}\"\n",
    "French:\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# Compose prompt → model\n",
    "chain = prompt_tmpl | llm\n",
    "\n",
    "# Invoke the chain with variables\n",
    "out = chain.invoke({\n",
    "    \"sentence\": \"Where is the nearest supermarket?\"\n",
    "})\n",
    "\n",
    "# Optional: show the rendered prompt for debugging / logging\n",
    "rendered_prompt = prompt_tmpl.format(\n",
    "    sentence=\"Where is the nearest supermarket?\"\n",
    ")\n",
    "\n",
    "print(f\"prompt:\\n{rendered_prompt}\\n\")\n",
    "print(f\"response:\\n{out.content}\\n\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "Human: Here is an example of translating a sentence from English to French:\n",
      "\n",
      "English: \"How is the weather today?\"\n",
      "French: \"Comment est le temps aujourd'hui?\"\n",
      "\n",
      "Now, translate the following sentence from English to French:\n",
      "\n",
      "English: \"Where is the nearest supermarket?\"\n",
      "French:\n",
      "\n",
      "\n",
      "response:\n",
      "French: \"Où est le supermarché le plus proche ?\"\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's response shows how it applies the structure and context provided by the initial example to translate the new sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider experimenting with different sentences or adjusting the parameters to see how these changes impact the model's translations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of few-shot learning by classifying emotions from text statements. \n",
    "\n",
    "Let's provide the model with three examples, each labeled with an appropriate emotion—joy, frustration, and sadness—to establish a pattern or guideline on how to categorize emotions in statements.\n",
    "\n",
    "After presenting these examples, let's challenge the model with a new statement: \"That movie was so scary I had to cover my eyes.\" The task for the model is to classify the emotion expressed in this new statement based on the learning from the provided examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:30:18.033693Z",
     "start_time": "2025-12-22T20:30:17.493234Z"
    }
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_tmpl = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Return ONLY the emotion label (one word). No punctuation, no extra text.\"),\n",
    "    (\"user\", \"\"\"Here are few examples of classifying emotions in statements:\n",
    "\n",
    "Statement: \"I just won my first marathon!\"\n",
    "Emotion: Joy\n",
    "\n",
    "Statement: \"I can't believe I lost my keys again.\"\n",
    "Emotion: Frustration\n",
    "\n",
    "Statement: \"My best friend is moving to another country.\"\n",
    "Emotion: Sadness\n",
    "\n",
    "Now, classify the emotion in the following statement:\n",
    "Statement: \"{statement}\"\n",
    "Emotion:\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "chain = prompt_tmpl | llm | StrOutputParser()\n",
    "\n",
    "statement = \"That movie was so scary I had to cover my eyes.\"\n",
    "\n",
    "emotion = chain.invoke({\"statement\": statement}).strip()\n",
    "\n",
    "print(\"Rendered prompt:\\n\", prompt_tmpl.format(statement=statement), \"\\n\")\n",
    "print(\"Raw emotion repr:\", repr(emotion))\n",
    "print(\"Emotion:\", emotion if emotion else \"<EMPTY>\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendered prompt:\n",
      " System: Return ONLY the emotion label (one word). No punctuation, no extra text.\n",
      "Human: Here are few examples of classifying emotions in statements:\n",
      "\n",
      "Statement: \"I just won my first marathon!\"\n",
      "Emotion: Joy\n",
      "\n",
      "Statement: \"I can't believe I lost my keys again.\"\n",
      "Emotion: Frustration\n",
      "\n",
      "Statement: \"My best friend is moving to another country.\"\n",
      "Emotion: Sadness\n",
      "\n",
      "Now, classify the emotion in the following statement:\n",
      "Statement: \"That movie was so scary I had to cover my eyes.\"\n",
      "Emotion:\n",
      " \n",
      "\n",
      "Raw emotion repr: 'Fear'\n",
      "Emotion: Fear\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are set with `max_new_tokens` to 10, which constrains the model to generate brief responses, focusing on the essential output without elaboration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's response demonstrates its ability to use the provided few examples to understand and classify the emotion of the new statement effectively following the same pattern in examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-thought (CoT) prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the Chain-of-Thought (CoT) prompting technique, designed to guide the model through a sequence of reasoning steps to solve a problem. In this example, the problem is a simple arithmetic question: “A store had 22 apples. They sold 15 apples today and received a new delivery of 8 apples. How many apples are there now?”\n",
    "\n",
    "The CoT technique involves structuring the prompt by instructing the model to “Break down each step of your calculation.” This encourages the model to include explicit reasoning steps, mimicking human-like problem-solving processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the response of the model, you can see the prompt directs the model to:\n",
    "\n",
    "1. Add the initial number of apples to the apples received in the new delivery.\n",
    "2. Subtract the number of apples sold from the sum obtained in the first step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By breaking down the problem into specific steps, the model is better able to understand the sequence of operations required to arrive at the correct answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-consistency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the self-consistency technique in reasoning through multiple calculations for a single problem. The problem posed is: “When I was 6, my sister was half my age. Now I am 70, what age is my sister?”\n",
    "\n",
    "The prompt instructs, “Provide three independent calculations and explanations, then determine the most consistent result.” This encourages the model to engage in critical thinking and consistency checking, which are vital for complex decision-making processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:38:33.067710Z",
     "start_time": "2025-12-22T20:38:25.704942Z"
    }
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt_tmpl = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a careful reasoning assistant.\"),\n",
    "    (\"user\", \"\"\"Consider the problem:\n",
    "\n",
    "{problem}\n",
    "\n",
    "Provide three independent calculations and explanations (label them Calculation 1, 2, 3).\n",
    "Then determine the most consistent final answer and clearly state it as:\n",
    "Final Answer: <value>\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# IMPORTANT: override max_tokens for this run (don’t rely on whatever llm was created with)\n",
    "llm_long = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5, max_tokens=800)\n",
    "\n",
    "chain = prompt_tmpl | llm_long | StrOutputParser()\n",
    "\n",
    "problem = \"A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. How many apples are there now?\"\n",
    "text = chain.invoke({\"problem\": problem})\n",
    "\n",
    "print(\"Chars:\", len(text))\n",
    "print(text)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chars: 1262\n",
      "Let's break down the problem with three independent calculations.\n",
      "\n",
      "### Calculation 1:\n",
      "1. Start with the initial number of apples: 22.\n",
      "2. Subtract the number of apples sold: 22 - 15 = 7.\n",
      "3. Add the new delivery of apples: 7 + 8 = 15.\n",
      "\n",
      "**Explanation**: After selling 15 apples from the original 22, there are 7 apples left. With the delivery of 8 new apples, the total becomes 15.\n",
      "\n",
      "### Calculation 2:\n",
      "1. Start with the initial number of apples: 22.\n",
      "2. Calculate the total number of apples sold and delivered: 15 (sold) - 8 (delivered) = -7.\n",
      "3. Add this net change to the initial number of apples: 22 + (-7) = 15.\n",
      "\n",
      "**Explanation**: By considering the net effect of selling and receiving apples, we find that the store ends up with 15 apples after accounting for both the sales and the new delivery.\n",
      "\n",
      "### Calculation 3:\n",
      "1. Start with the initial number of apples: 22.\n",
      "2. Directly subtract the apples sold: 22 - 15 = 7.\n",
      "3. Then add the new delivery: 7 + 8 = 15.\n",
      "\n",
      "**Explanation**: This calculation follows the same logic as Calculation 1, confirming that after selling 15 apples and receiving 8, the total is 15 apples.\n",
      "\n",
      "### Consistency Check:\n",
      "All three calculations lead to the same result of 15 apples remaining in the store after the transactions.\n",
      "\n",
      "Final Answer: 15\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's response shows that it provides three different calculations and explanations. Each calculation attempts to derive the sister's age using different logical approaches.\n",
    "\n",
    "Self-consistency can help identify the most accurate and reliable answer in scenarios where multiple plausible solutions exist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will show you how to use the prompt template from Langchain to create more structured and reproducible prompts. You will also learn to create some applications based on the prompt template.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prompt template](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates) is a key concept in langchain, it helps to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the prompt template, you need to initialize a LLM first.\n",
    "\n",
    "You can still use the `ibm/granite-3-2-8b-instruct` from watsonx.ai.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_id = 'ibm/granite-3-2-8b-instruct'\n\nparameters = {\n    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n}\n\ncredentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\"\n}\n\nproject_id = \"skills-network\"\n\nmodel = Model(\n    model_id=model_id,\n    params=parameters,\n    credentials=credentials,\n    project_id=project_id\n)\n\nmixtral_llm = WatsonxLLM(model=model)\nmixtral_llm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `PromptTemplate` to create a template for a string-based prompt. In this template, you'll define two parameters: `adjective` and `content`. These parameters allow for the reuse of the prompt across different situations. For instance, to adapt the prompt to various contexts, simply pass the relevant values to these parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "template = \"\"\"Tell me a {adjective} joke about {content}.\n\"\"\"\nprompt = PromptTemplate.from_template(template)\nprompt "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at how the prompt has been formatted.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "prompt.format(adjective=\"funny\", content=\"chickens\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the response, you can see that the prompt is formatted according to the specified context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will wrap the formatted prompt into the LLMChain, and then invoke the prompt to get the response from the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm)\nresponse = llm_chain.invoke(input = {\"adjective\": \"funny\", \"content\": \"chickens\"})\nprint(response[\"text\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the response, you can see the LLM came up with a funny joke about chickens.\n",
    "\n",
    "To use this prompt in another context, simply replace the variables accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "response = llm_chain.invoke(input = {\"adjective\": \"sad\", \"content\": \"fish\"})\nprint(response[\"text\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, you will learn how to create agents capable of completing various tasks using prompt templates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text summarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a text summarization agent designed to help summarize the content you provide to the LLM. \n",
    "\n",
    "You can store the content to be summarized in a variable, allowing for repeated use of the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "content = \"\"\"\n        The rapid advancement of technology in the 21st century has transformed various industries, including healthcare, education, and transportation. \n        Innovations such as artificial intelligence, machine learning, and the Internet of Things have revolutionized how we approach everyday tasks and complex problems. \n        For instance, AI-powered diagnostic tools are improving the accuracy and speed of medical diagnoses, while smart transportation systems are making cities more efficient and reducing traffic congestion. \n        Moreover, online learning platforms are making education more accessible to people around the world, breaking down geographical and financial barriers. \n        These technological developments are not only enhancing productivity but also contributing to a more interconnected and informed society.\n\"\"\"\n\ntemplate = \"\"\"Summarize the {content} in one sentence.\n\"\"\"\nprompt = PromptTemplate.from_template(template)\n\nllm_chain = LLMChain(prompt=prompt, llm=mixtral_llm)\nresponse = llm_chain.invoke(input = {\"content\": content})\nprint(response[\"text\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question answering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a Q&A agent. \n",
    "\n",
    "This agent enables the LLM to learn from the provided content and answer questions based on what it has learned. Occasionally, if the LLM does not have sufficient information, it might generate a speculative answer. To manage this, you'll specifically instruct it to respond with \"Unsure about the answer\" if it is uncertain about the correct response.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "content = \"\"\"\n        The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. \n        The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. \n        The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n\"\"\"\n\nquestion = \"Which planets in the solar system are rocky and solid?\"\n\ntemplate = \"\"\"\n            Answer the {question} based on the {content}.\n            Respond \"Unsure about answer\" if not sure about the answer.\n            \n            Answer:\n            \n\"\"\"\nprompt = PromptTemplate.from_template(template)\noutput_key = \"answer\"\n\nllm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)\nresponse = llm_chain.invoke(input = {\"question\":question ,\"content\": content})\nprint(response[\"answer\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a text classification agent designed to categorize text into predefined categories. This example employs zero-shot learning, where the agent classifies text without prior exposure to related examples.\n",
    "\n",
    "Can you revise it to the one-shot learning or few-shot learning in the exercises?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:36:13.061690Z",
     "start_time": "2025-12-22T20:36:13.047745Z"
    }
   },
   "source": [
    "text = \"\"\"\n        The concert last night was an exhilarating experience with outstanding performances by all artists.\n\"\"\"\n\ncategories = \"Entertainment, Food and Dining, Technology, Literature, Music.\"\n\ntemplate = \"\"\"\n            Classify the {text} into one of the {categories}.\n            \n            Category:\n            \n\"\"\"\nprompt = PromptTemplate.from_template(template)\noutput_key = \"category\"\n\nllm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)\nresponse = llm_chain.invoke(input = {\"text\":text ,\"categories\": categories})\nprint(response[\"category\"])"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 13\u001B[0m\n\u001B[1;32m      5\u001B[0m categories \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEntertainment, Food and Dining, Technology, Literature, Music.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m template \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m            Classify the \u001B[39m\u001B[38;5;132;01m{text}\u001B[39;00m\u001B[38;5;124m into one of the \u001B[39m\u001B[38;5;132;01m{categories}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m            \u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124m            Category:\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124m            \u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m---> 13\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[43mPromptTemplate\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_template(template)\n\u001B[1;32m     14\u001B[0m output_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategory\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     16\u001B[0m llm_chain \u001B[38;5;241m=\u001B[39m LLMChain(prompt\u001B[38;5;241m=\u001B[39mprompt, llm\u001B[38;5;241m=\u001B[39mmixtral_llm, output_key\u001B[38;5;241m=\u001B[39moutput_key)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'PromptTemplate' is not defined"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of an SQL code generation agent. This agent is designed to generate SQL queries based on given descriptions. It interprets the requirements from your input and translates them into executable SQL code.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:36:09.687614Z",
     "start_time": "2025-12-22T20:36:09.675981Z"
    }
   },
   "source": [
    "description = \"\"\"\n        Retrieve the names and email addresses of all customers from the 'customers' table who have made a purchase in the last 30 days. \n        The table 'purchases' contains a column 'purchase_date'\n\"\"\"\n\ntemplate = \"\"\"\n            Generate an SQL query based on the {description}\n            \n            SQL Query:\n            \n\"\"\"\nprompt = PromptTemplate.from_template(template)\noutput_key = \"query\"\n\nllm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)\nresponse = llm_chain.invoke(input = {\"description\":description})\nprint(response[\"query\"])"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 12\u001B[0m\n\u001B[1;32m      1\u001B[0m description \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124m        Retrieve the names and email addresses of all customers from the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcustomers\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m table who have made a purchase in the last 30 days. \u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124m        The table \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpurchases\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m contains a column \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpurchase_date\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      6\u001B[0m template \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124m            Generate an SQL query based on the \u001B[39m\u001B[38;5;132;01m{description}\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m            \u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m            SQL Query:\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124m            \u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m---> 12\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[43mPromptTemplate\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_template(template)\n\u001B[1;32m     13\u001B[0m output_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     15\u001B[0m llm_chain \u001B[38;5;241m=\u001B[39m LLMChain(prompt\u001B[38;5;241m=\u001B[39mprompt, llm\u001B[38;5;241m=\u001B[39mmixtral_llm, output_key\u001B[38;5;241m=\u001B[39moutput_key)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'PromptTemplate' is not defined"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role playing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also configure the LLM to assume specific roles as defined by us, enabling it to follow predetermined rules and behave like a task-oriented chatbot.\n",
    "\n",
    "For example, the code below configures the LLM to act as a game master. In this role, the LLM answers questions about games while maintaining an engaging and immersive tone, enhancing the user experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to create the prompt template and create a LLMChian to wrap the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:36:03.422331Z",
     "start_time": "2025-12-22T20:36:03.398685Z"
    }
   },
   "source": [
    "role = \"\"\"\n        game master\n\"\"\"\n\ntone = \"engaging and immersive\"\n\ntemplate = \"\"\"\n            You are an expert {role}. I have this question {question}. I would like our conversation to be {tone}.\n            \n            Answer:\n            \n\"\"\"\nprompt = PromptTemplate.from_template(template)\noutput_key = \"answer\"\n\nllm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 13\u001B[0m\n\u001B[1;32m      5\u001B[0m tone \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mengaging and immersive\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m template \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124m            You are an expert \u001B[39m\u001B[38;5;132;01m{role}\u001B[39;00m\u001B[38;5;124m. I have this question \u001B[39m\u001B[38;5;132;01m{question}\u001B[39;00m\u001B[38;5;124m. I would like our conversation to be \u001B[39m\u001B[38;5;132;01m{tone}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m            \u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124m            Answer:\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124m            \u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m---> 13\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[43mPromptTemplate\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_template(template)\n\u001B[1;32m     14\u001B[0m output_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manswer\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     16\u001B[0m llm_chain \u001B[38;5;241m=\u001B[39m LLMChain(prompt\u001B[38;5;241m=\u001B[39mprompt, llm\u001B[38;5;241m=\u001B[39mmixtral_llm, output_key\u001B[38;5;241m=\u001B[39moutput_key)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'PromptTemplate' is not defined"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will create a game master chatbot that takes your questions as input and provides responses from the model.\n",
    "\n",
    "Run the code below to launch the bot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the bot by asking the question, \"Who are you?\" The bot will respond with \"I am a game master,\" indicating it has assumed the role that you predefined.\n",
    "\n",
    "The function is written within a while loop, allowing continuous interaction. To exit the loop and terminate the conversation, type \"quit,\" \"exit,\" or \"bye\" into the input box.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "while True:\n    query = input(\"Question: \")\n    \n    if query.lower() in [\"quit\",\"exit\",\"bye\"]:\n        print(\"Answer: Goodbye!\")\n        break\n        \n    response = llm_chain.invoke(input = {\"role\": role, \"question\": query, \"tone\": tone})\n    \n    print(\"Answer: \", response[\"answer\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You finish the lab. Now let's take some exercises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Change parameters for the LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with changing the parameters of the LLM to observe how different settings impact the responses. Adjusting parameters such as `max_new_tokens`, `temperature`, or `top_p` can significantly alter the behavior of the model. Try different configurations to see how each variation influences the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"min_new_tokens\": 100,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.1,\n",
    "    \"top_k\": 1\n",
    "}\n",
    "\n",
    "prompt = \"The wind is\"\n",
    "\n",
    "response = llm_model(prompt, params)\n",
    "print(response)\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Observe how LLM thinks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set `verbose=True` in the `LLMChain()` to observe the thought process of the LLM, gaining insights into how it formulates its responses. Can you make it any agent you created before to observe it?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "content = \"\"\"\n",
    "        The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. \n",
    "        The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. \n",
    "        The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which planets in the solar system are rocky and solid?\"\n",
    "\n",
    "template = \"\"\"\n",
    "            Answer the {question} based on the {content}.\n",
    "            Respond \"Unsure about answer\" if not sure about the answer.\n",
    "            \n",
    "            Answer:\n",
    "            \n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "output_key = \"answer\"\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key, verbose=True)\n",
    "response = llm_chain.invoke(input = {\"question\":question ,\"content\": content})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Revise the text classification agent to one-shot learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You were using zero-shot learning when you created the text classification agent. Can you revise it to use one-shot learning?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "example_text = \"\"\"\n",
    "               Last week's book fair was a delightful gathering of authors and readers, featuring discussions and book signings.\n",
    "               \"\"\"\n",
    "\n",
    "example_category = \"Literature\"\n",
    "\n",
    "text = \"\"\"\n",
    "       The concert last night was an exhilarating experience with outstanding performances by all artists.\n",
    "       \"\"\"\n",
    "\n",
    "categories = \"Entertainment, Food and Dining, Technology, Literature, Music.\"\n",
    "\n",
    "template = \"\"\"\n",
    "           Example:\n",
    "           Text: {example_text}\n",
    "           Category: {example_category}\n",
    "\n",
    "           Now, classify the following text into one of the specified categories: {categories}\n",
    "           \n",
    "           Text: {text}\n",
    "           \n",
    "           Category:\n",
    "           \n",
    "           \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "output_key = \"category\"\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)\n",
    "response = llm_chain.invoke(input = {\"example_text\": example_text, \"example_category\":example_category ,\"categories\": categories, \"text\":text})\n",
    "print(response[\"category\"])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kang Wang](https://author.skills.network/instructors/kang_wang)\n",
    "\n",
    "Kang Wang is a Data Scientist in IBM. He is also a PhD Candidate in the University of Waterloo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "913053b97989ac7f4c567f289accb0afa6955eb021c718006d7ca35d184bb3f5"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
